[{"content":"1、准备环境 监控系统下载运行prometheus和alertmanager，被监控的ES集群下载运行node_exporter和elasticsearch_exporter\n2、修改配置 1、prometheus配置： # my global config global: scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute. evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s). # Alertmanager configuration alerting: alertmanagers: - static_configs: - targets: - s4:9093 # Load rules once and periodically evaluate them according to the global 'evaluation_interval'. rule_files: - \u0026quot;alerts/*.yml\u0026quot; # A scrape configuration containing exactly one endpoint to scrape: # Here it's Prometheus itself. scrape_configs: # The job name is added as a label `job=\u0026lt;job_name\u0026gt;` to any timeseries scraped from this config. - job_name: 'prometheus' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. static_configs: - targets: ['localhost:9090'] - job_name: 'S5' static_configs: - targets: ['10.3.3.5:9100'] - job_name: 'S6_mariadb' static_configs: - targets: ['10.3.3.6:9104'] - job_name: 'S5_elasticsearch' scrape_interval: 60s scrape_timeout: 30s metrics_path: \u0026quot;/metrics\u0026quot; static_configs: - targets: - '10.3.3.5:9108' labels: service: elasticsearch relabel_configs: - source_labels: [__address__] regex: '(.*)\\:9109' target_label: 'instance' replacement: '$1' - source_labels: [__address__] regex: '.*\\.(.*)\\.lan.*' target_label: 'environment' replacement: '$1'  2、alertmanager配置 alertmanager.yml\nglobal: # 当Alertmanager持续多长时间未接收到告警后标记告警状态为resolved（已解决），默认为5m resolve_timeout: 5m # 全局的SMTP服务器配置 smtp_smarthost: 'smtp.126.com:25' smtp_from: 'maple34@126.com' smtp_auth_username: 'maple34@126.com' smtp_auth_password: '{{password}}' route: group_by: ['alertname'] # 有的时候为了能够一次性收集和发送更多的相关信息时，可以通过group_wait参数设置等待时间， # 如果在等待时间内当前group接收到了新的告警，这些告警将会合并为一个通知向receiver发送，默认为30s group_wait: 10s # 定义相同的Group之间发送告警通知的时间间隔，默认为5m group_interval: 20s # 如果已经成功发送了一个警告，在再次发送通知之前需要等待多长时间，默认为4h repeat_interval: 1m receiver: 'mail-error' routes: - match: level: '严重' receiver: 'mail-error' - match: level: '紧急' receiver: 'mail-warning' repeat_interval: 10m receivers: - name: 'mail-error' email_configs: - to: 345999369@qq.com - name: 'mail-warning' email_configs: - to: maple34@126.com  3、alertmanager告警规则配置 在prometheus监控主机的prometheus根目录下，建立rules文件夹，放置告警规则配置文件。\n[admin@s4 prometheus-2.5.0]$ ls rules/ es_alert.yml os_alert.yml  OS存活状态和内存使用率： OS宕机或系统内存使用率大于70%触发告警\nos_alert.yml\ngroups: - name: OS_alert rules: - alert: InstanceDown expr: up == 0 for: 1m labels: severity: critical annotations: summary: \u0026quot;Instance {{ $labels.instance }} down\u0026quot; description: \u0026quot;{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 1 minutes.\u0026quot; - alert: NodeMemoryUsage expr: (node_memory_MemTotal_bytes - (node_memory_MemFree_bytes+node_memory_Buffers_bytes+node_memory_Cached_bytes )) / node_memory_MemTotal_bytes * 100 \u0026gt; 70 for: 1m labels: severity: critical annotations: summary: \u0026quot;{{$labels.instance}}: High Memory usage detected\u0026quot; description: \u0026quot;{{$labels.instance}}: Memory usage is above 70% (current value is:{{ $value }})\u0026quot;  ES集群多个指标不正常触发告警： es_alert.yml\ngroups: - name: ES_Alert rules: ########## 集群健康状态：红色 ############### - alert: Elastic_Cluster_Health_RED expr: elasticsearch_cluster_health_status{color=\u0026quot;red\u0026quot;}==1 for: 1m labels: severity: critical annotations: summary: \u0026quot;Instance {{ $labels.instance }}: not all primary and replica shards are allocated in elasticsearch cluster {{ $labels.cluster }}\u0026quot; description: \u0026quot;Instance {{ $labels.instance }}: not all primary and replica shards are allocated in elasticsearch cluster {{ $labels.cluster }}.\u0026quot; ########## 集群健康状态：黄色 ############### - alert: Elastic_Cluster_Health_Yellow expr: elasticsearch_cluster_health_status{color=\u0026quot;yellow\u0026quot;}==1 for: 1m labels: severity: critical annotations: summary: \u0026quot; Instance {{ $labels.instance }}: not all primary and replica shards are allocated in elasticsearch cluster {{ $labels.cluster }}\u0026quot; description: \u0026quot;Instance {{ $labels.instance }}: not all primary and replica shards are allocated in elasticsearch cluster {{ $labels.cluster }}.\u0026quot; ########## ES JVM堆内存使用率超过百分之80 ############### - alert: Elasticsearch_JVM_Heap_Too_High expr: elasticsearch_jvm_memory_used_bytes{area=\u0026quot;heap\u0026quot;} / elasticsearch_jvm_memory_max_bytes{area=\u0026quot;heap\u0026quot;} \u0026gt; 0.8 for: 1m labels: severity: critical annotations: summary: \u0026quot;ElasticSearch node {{ $labels.instance }} heap usage is high \u0026quot; description: \u0026quot;The heap in {{ $labels.instance }} is over 80% for 15m.\u0026quot; - alert: Elasticsearch_health_up expr: elasticsearch_cluster_health_up !=1 for: 1m labels: severity: critical annotations: summary: \u0026quot; ElasticSearch node: {{ $labels.instance }} last scrape of the ElasticSearch cluster health failed\u0026quot; description: \u0026quot;ElasticSearch node: {{ $labels.instance }} last scrape of the ElasticSearch cluster health failed\u0026quot; - alert: Elasticsearch_Count_of_JVM_GC_Runs expr: rate(elasticsearch_jvm_gc_collection_seconds_count{}[5m])\u0026gt;5 for: 1m labels: severity: critical annotations: summary: \u0026quot;ElasticSearch node {{ $labels.instance }}: Count of JVM GC runs \u0026gt; 5 per sec and has a value of {{ $value }} \u0026quot; description: \u0026quot;ElasticSearch node {{ $labels.instance }}: Count of JVM GC runs \u0026gt; 5 per sec and has a value of {{ $value }}\u0026quot; - alert: Elasticsearch_GC_Run_Time expr: rate(elasticsearch_jvm_gc_collection_seconds_sum[5m])\u0026gt;0.3 for: 1m labels: severity: critical annotations: summary: \u0026quot; ElasticSearch node {{ $labels.instance }}: GC run time in seconds \u0026gt; 0.3 sec and has a value of {{ $value }}\u0026quot; description: \u0026quot;ElasticSearch node {{ $labels.instance }}: GC run time in seconds \u0026gt; 0.3 sec and has a value of {{ $value }}\u0026quot; - alert: Elasticsearch_health_timed_out expr: elasticsearch_cluster_health_timed_out\u0026gt;0 for: 1m labels: severity: critical annotations: summary: \u0026quot; ElasticSearch node {{ $labels.instance }}: Number of cluster health checks timed out \u0026gt; 0 and has a value of {{ $value }}\u0026quot; description: \u0026quot;ElasticSearch node {{ $labels.instance }}: Number of cluster health checks timed out \u0026gt; 0 and has a value of {{ $value }}\u0026quot;  ","date":"2021-02-06","permalink":"https://richard-chen93.github.io/post/%E4%BD%BF%E7%94%A8prometheus%E7%9B%91%E6%8E%A7es%E9%9B%86%E7%BE%A4/","tags":["technology"],"title":"使用Prometheus监控ES集群"},{"content":"1、安装yum工具 yum -y install createrepo  2、指定yum仓库路径 mkdir /repo  3、创建本地repo仓库 createrepo -pdo /repo /repo  4、清空或者备份出 /etc/yum.repos.d 下所有的源。 mv /etc/yum.repos.d/*.repo /tmp yum clean all  5、配置本地yum源 cat \u0026gt;\u0026gt; /etc/yum.repos.d/Centos-Base.repo \u0026lt;\u0026lt; EOF [centos7] name=CentOS7-local baseurl=file:///repo gpgcheck=0 enabled=1 EOF  6、测试 yum makecache yum -y install unzip  ","date":"2021-01-25","permalink":"https://richard-chen93.github.io/post/linux%E6%90%AD%E5%BB%BA%E5%86%85%E7%BD%91yum%E6%BA%90/","tags":["technology"],"title":"Linux搭建内网yum源"},{"content":"1、环境 es版本：\t7.8.1\ncentos7： s4,s5,s6\n配置：4c 3G\n3台机器必须调整最大进程数，root执行：\necho \u0026quot;fs.file-max = 2000000\u0026quot; \u0026gt;\u0026gt; /etc/sysctl.conf echo \u0026quot;vm.max_map_count = 655360\u0026quot; \u0026gt;\u0026gt; /etc/sysctl.conf sysctl -p echo \u0026quot;ulimit -u 10000\u0026quot; \u0026gt;\u0026gt; /etc/profile  2、配置 elasticsearch.yml配置文件。3台机器的node.name分别修改为：node-1,node-2,node-3。其余不变。\n#集群名称 cluster.name: test-app #节点名称，集群中保持唯一 node.name: node-1 node.master: true #绑定远程地址，为了安全通常是指定具体的地址，这里仅仅是测试，放开允许所有远程来源访问 network.host: 0.0.0.0 #开放http接口，默认就是9200 http.port: 9200 #集群节点之间（集群协商、指令传输等）通信的端口 transport.tcp.port: 9300 #允许前端跨域访问 http.cors.enabled: true #设置允许的跨域的来源，*表示允许所有跨域来源 http.cors.allow-origin: \u0026quot;*\u0026quot; #设置发现集群节点主机列表 discovery.seed_hosts: [\u0026quot;s4:9300\u0026quot;,\u0026quot;s5:9300\u0026quot;,\u0026quot;s6:9300\u0026quot;] #初始化集群的master节点的候选列表，列表中的节点都可能竞选成为master节点 cluster.initial_master_nodes: [\u0026quot;s4:9300\u0026quot;,\u0026quot;s5:9300\u0026quot;,\u0026quot;s6:9300\u0026quot;]  3、启动并验证 ./bin/elasticsearch\n查看集群信息：http://s4:9200/_cat/health?v\n","date":"2021-01-24","permalink":"https://richard-chen93.github.io/post/elasticsearch%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/","tags":["technology"],"title":"Elasticsearch集群搭建"},{"content":"前言 centos7系统，最小化安装系统后：关闭防火墙和selinux、安装常用工具、设置yum源、创建普通用户admin、cephuser，设置ntp、系统和进程级别的文件句柄\n1、一键搞定脚本 #!/bin/bash ##########optimize system cat \u0026gt;\u0026gt; /etc/security/limits.conf \u0026lt;\u0026lt; EOF * soft noproc 65535 * hard noproc 65535 * soft nofile 1000000 * hard nofile 1000000 EOF echo \u0026quot;fs.file-max = 2000000\u0026quot; \u0026gt;\u0026gt; /etc/sysctl.conf echo \u0026quot;vm.max_map_count=655360\u0026quot; \u0026gt;\u0026gt; /etc/sysctl.conf sysctl -p echo \u0026quot;ulimit -u 10000\u0026quot; \u0026gt;\u0026gt; /etc/profile source /etc/profile cd #############disable and stop selinux \u0026amp; firewalld sed -i 's/#UseDNS yes/UseDNS no/g' /etc/ssh/sshd_config sed -i 's/GSSAPIAuthentication yes/GSSAPIAuthentication no/g' /etc/ssh/sshd_config sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/ssh/sshd_config setenforce 0 systemctl diable firewalld systemctl stop firewalld ###########set aliyun mirror yum -y install wget wget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo sed -i -e '/mirrors.cloud.aliyuncs.com/d' -e '/mirrors.aliyuncs.com/d' /etc/yum.repos.d/CentOS-Base.repo wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo yum makecache ############install common soft yum -y install epel-release git ntp ntpdate curl vim net-tools python36 python-pip lrzsz yum -y groupinstall \u0026quot;Fonts\u0026quot; ############### add chinese language echo 'export LC_ALL=\u0026quot;zh_CN.UTF-8\u0026quot;' \u0026gt;\u0026gt; /etc/profile source /etc/profile ############### add common users echo \u0026quot;admin ALL = (root) NOPASSWD:ALL\u0026quot; | sudo tee /etc/sudoers.d/admin sudo useradd cephuser ; echo cephuser | sudo passwd --stdin cephuser echo \u0026quot;cephuser ALL = (root) NOPASSWD:ALL\u0026quot; | sudo tee /etc/sudoers.d/cephuser sudo chmod 0440 /etc/sudoers.d/cephuser ################# make ceph yum repo cat \u0026gt; /etc/yum.repos.d/ceph.repo \u0026lt;\u0026lt;EOF [ceph-luminous-noarch] name = ceph-luminous-noarch baseurl = https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-luminous/el7/noarch/ enabled = 1 gpgcheck = 0 gkgkey = http://mirrors.tuna.tsinghua.edu.cn/ceph/keys/release.asc [ceph-luminous-x64] name = ceph-luminous-x64 baseurl = https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-luminous/el7/x86_64/ enabled = 1 gpgcheck = 0 gkgkey = http://mirrors.tuna.tsinghua.edu.cn/ceph/keys/release.asc EOF sudo yum makecache ################# install ceph needed pakages wget https://files.pythonhosted.org/packages/5f/ad/1fde06877a8d7d5c9b60eff7de2d452f639916ae1d48f0b8f97bf97e570a/distribute-0.7.3.zip sudo yum -y install unzip unzip distribute-0.7.3.zip cd distribute-0.7.3 sudo python setup.py install sudo yum -y install deltarpm   2、非脚本 设置最大进程数和句柄 添加用户级别句柄和进程 root执行\n  cat \u0026gt;\u0026gt; /etc/security/limits.conf \u0026lt;\u0026lt; EOF * soft noproc 65535 * hard noproc 65535 * soft nofile 1000000 * hard nofile 1000000 EOF sysctl -w fs.file-max =65536 echo \u0026quot;ulimit -u 10000\u0026quot; \u0026gt;\u0026gt; /etc/profile source /etc/profile cd    说明：* 代表针对所有用户\nnoproc 是代表最大进程数\nnofile 是代表最大文件打开数\n系统级别句柄\n设置各linux 用户的最大进程数，下面把某linux用户的最大进程数设为10000个：\n 常用的初始配置，加快dns，关闭selinux和防火墙 sed -i 's/#UseDNS yes/UseDNS no/g' /etc/ssh/sshd_config sed -i 's/GSSAPIAuthentication yes/GSSAPIAuthentication no/g' /etc/ssh/sshd_config sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config setenforce 0 systemctl disable firewalld systemctl stop firewalld  centos7:\nyum -y install wget wget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo sed -i -e '/mirrors.cloud.aliyuncs.com/d' -e '/mirrors.aliyuncs.com/d' /etc/yum.repos.d/CentOS-Base.repo yum makecache  安装常用组件 yum -y update yum -y install epel-release git ntp ntpdate curl vim net-tools python36 python-pip  pip: pip config set global.index-url https://mirrors.aliyun.com/pypi/simple/ pip install django-bootstrap3==6.2.2 -i http://mirrors.aliyun.com/pypi/simple --trusted-host=mirrors.aliyun.com  Mac: 修改 pip.conf 文件\nvim $HOME/Library/Application Support/pip/pip.conf  如果没有上面的目录,在如下目录创建 pip.conf\n$HOME/.config/pip/pip.conf\n修改内容如下：\n[global] index-url = https://pypi.tuna.tsinghua.edu.cn/simple12  Windows: 修改 pip.conf 文件 (没有就创建一个)\n%APPDATA%\\pip\\pip.ini\n修改内容如下：\n[global] index-url = https://pypi.tuna.tsinghua.edu.cn/simple  docker： Ubuntu 16.04+、Debian 8+、CentOS 7+ 创建或修改 /etc/docker/daemon.json：\nsudo mkdir -p /etc/docker sudo tee /etc/docker/daemon.json \u0026lt;\u0026lt;-'EOF' { \u0026quot;registry-mirrors\u0026quot;: [ \u0026quot;https://1nj0zren.mirror.aliyuncs.com\u0026quot;, \u0026quot;https://docker.mirrors.ustc.edu.cn\u0026quot;, \u0026quot;http://f1361db2.m.daocloud.io\u0026quot;, \u0026quot;https://registry.docker-cn.com\u0026quot; ] } EOF sudo systemctl daemon-reload sudo systemctl restart docker  Docker 中国官方镜像\nhttps://registry.docker-cn.com\nDocker Hub\nDaoCloud 镜像站\nhttp://f1361db2.m.daocloud.io\n可登录，系统分配\nDocker Hub\nAzure 中国镜像\nhttps://dockerhub.azk8s.cn\nDocker Hub、GCR、Quay\n科大镜像站\nhttps://docker.mirrors.ustc.edu.cn\nDocker Hub、GCR、Quay\n阿里云\nhttps://\u0026lt;your_code\u0026gt;.mirror.aliyuncs.com\n需登录，系统分配\nDocker Hub\n七牛云\nhttps://reg-mirror.qiniu.com\nDocker Hub、GCR、Quay\n网易云\nhttps://hub-mirror.c.163.com\nDocker Hub\n腾讯云\nhttps://mirror.ccs.tencentyun.com\nDocker Hub\n检查加速器是否生效 命令行执行 docker info，如果从结果中看到了如下内容，说明配置成功。\nRegistry Mirrors: [...] https://registry.docker-cn.com/  3、系统调优 limits.conf 和sysctl.conf区别在于limits.conf是针对用户，而sysctl.conf是针对整个系统参数配置。\nlimits.conf文件实际是Linux PAM（插入式认证模块，Pluggable Authentication Modules中pam_limits.so的配置文件），突破系统的默认限制，对系统访问资源有一定保护作用，当用户访问服务器时，服务程序将请求发送到PAM模块，PAM模块根据服务名称在/etc/pam.d目录下选择一个对应的服务文件，然后根据服务文件的内容选择具体的PAM模块进行处理。 limits.conf 说明：* 代表针对所有用户\nnoproc 是代表最大进程数\nnofile 是代表最大文件打开数\n* soft noproc 65535 * hard noproc 65535 * soft nofile 1000000 * hard nofile 1000000  /etc/sysctl.conf 查看系统级别最大文件句柄数\ncat /proc/sys/fs/file-max   修改最大文件句柄数和用户最大进程数：永久生效\n1、修改配置文件加入内核参数/etc/sysctl.conf\necho \u0026quot;fs.file-max = 2000000\u0026quot; \u0026gt;\u0026gt; /etc/sysctl.conf echo \u0026quot;vm.max_map_count = 655360\u0026quot; \u0026gt;\u0026gt; /etc/sysctl.conf sysctl -p  50W并发可设置 = 999999\n注：修改范围为系统所有进程可打开的最大文件句柄\n用户最大进程数 设置各linux 用户的最大进程数，下面把某linux用户的最大进程数设为60000个：\necho \u0026quot;ulimit -u 10000\u0026quot; \u0026gt;\u0026gt; /etc/profile  ","date":"2021-01-24","permalink":"https://richard-chen93.github.io/post/linux%E5%88%9D%E5%A7%8B%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E4%B8%8E%E7%B3%BB%E7%BB%9F%E8%B0%83%E4%BC%98/","tags":["technology"],"title":"Linux初始安装配置与系统调优"},{"content":"charles抓包工具的使用：手机抓包设置和安装证书 dufufd 2019-01-16 16:17:48 4645 收藏 4\n分类专栏： Web\nhttp://www.cnblogs.com/cnhkzyy/p/9535030.html\n一. 设置手机抓包 第一步：在charles里设置允许手机联网的权限，并设置接入接口 在Charles的菜单栏上选择\u0026quot;Proxy\u0026quot;-\u0026gt;\u0026ldquo;Proxy Settings\u0026rdquo;，填入代理端口8888（注意，这个端口不一定填写8888，也可以写别的端口），并且勾上”Enable transparent HTTP proxying”，这样就完成了在Charles上的设置\n在\u0026quot;Help\u0026quot;-\u0026gt;\u0026ldquo;Local IP Address\u0026quot;中可以查看本机的ip地址，当然也可以在cmd中通过ipconfig查看\n第二步：设置手机代理 以荣耀8为例，选中wifi名字，右击，选择修改网络\n显示高级选项，输入服务器主机名和服务器端口，点击保存\n二. 为避免(PC/Phone)抓取HTTPS失败和乱码，需要下载安装SSL/HTTPS证书 ==========================电脑端=============================\n第一步：电脑安装SSL证书 选择 \u0026ldquo;Help\u0026rdquo; -\u0026gt; \u0026ldquo;SSL Proxying\u0026rdquo; -\u0026gt; \u0026ldquo;Install Charles Root Certificate\u0026rdquo;，如果设置了安全防护，会ranging输入系统的帐号密码\n这时开始安装charles证书，一路点击下一步即可\n第二步：配置SSL的抓取域名 找到\u0026quot;Proxy\u0026rdquo;-\u0026gt;\u0026ldquo;SSL Proxying Settings\u0026hellip;\u0026quot;，点击\n然后选中启用SSL代理(Enable SSL Proxying)，charles的Location配置都是支持通配符的，因此在Host里设置一个\u0026rdquo;*\u0026ldquo;就可以，port不写\n如果需要配置某个指定域名，也是在Host里填写，配置指定域名时，一般Port是443，这样就可以抓取到到HTTPS的内容了\n==========================手机端=============================\n第一步：手机安装SSL证书 进入\u0026quot;Help\u0026rdquo;-\u0026gt;\u0026ldquo;Install Charles Root Certificate on a Mobile Device or remote Browser\u0026rdquo;，点击\n这时会有一个弹框，意思是要给手机设置代理，内容是192.168.1.103:8888，然后用手机浏览器打开chls.pro/ssl\n点击立即下载\n在手机设置-\u0026gt;高级设置-\u0026gt;安全里开启未知来源应用下载和外部来源应用安装\n在手机文件管理里找到证书，将后缀pem改成crt，点击安装即可\n这时发现手机上的HTTPS也能抓取下来了\n","date":"2021-01-20","permalink":"https://richard-chen93.github.io/post/charles%E6%89%8B%E6%9C%BA%E6%8A%93%E5%8C%85/","tags":["technology"],"title":"Charles手机抓包"},{"content":"「已注销」 2017-11-22 15:18:45 2697 收藏 2\n分类专栏： 随笔 文章标签： cassandra 集群\n1、集群部署 一：前提 安装jdk1.8以上，python2.7 二：安装Cassandra Cassandra的下载地址：http://cassandra.apache.org/download/ 下载后将文件解压到某目录下， 然后配置环境变量 CASSANDRA_HOME为你解压的目录， path为%CASSANDRA_HOME%\\bin 然后用管理员身份运行cmd（不然可能提示权限不够） 进入Cassandra目录下的bin， 执行cassandra 然后如果成功会出一大堆东西，并且不能再输入命令； 三：查询状态 再打开一个cmd窗口，原来的不要关闭 进入bin文件夹 执行nodetool status 这是成功状态， 然后输入cqlsh进入编写sql 如果执行cqlsh时出现Can't detect python version需要到pylib目录下执行python setup.py install\n出现cqlsh\u0026gt;开头就表示你现在正在编写sql； 四：查询命令 查看表空间 describe keyspaces； 查看已有表：describe tables; 查看表结构：describe table table_name;\n以上是单个几点的安装，下面是多个节点的集群部署： 修改配置文件：cassandra.yaml cluster_name：集群名称。 如果启动过数据库再修改集群名称需要先执行命令: 进入cqlsh执行 UPDATE system.local SET cluster_name = '你修改后的名称' where key='local'; 退出cqlsh状态，执行nodetool flush system seeds节点，将每个节点的ip加进去，\u0026quot;x.x.x.x,xx.xx.xx.xx\u0026quot;不用加尖括号！ listen_address改为自己的ip地址 rpc_address改为自己的ip地址 重启数据库。 再次执行cqlsh命令，后面需要加自己的ip\n2、监控 通过MX4J HTTP 适配器 健康 cassandra\n配置步骤如下：\n  下载最新的MX4J binary(e.g. mx4j-3.0.2.tar.gz):[ 下载](https://sourceforge.net/projects/mx4j/files/MX4J Binary/3.0.2/?SetFreedomCookie)\n  解压缩，吧mx4j-tools.jar 文件(在压缩包的 /lib/mx4j-tools.jar)复制到Cassandra的lib文件夹里(e.g. /usr/share/cassandra/lib)\n  在Cassandra的配置文件cassandra-env.sh文件中，在最下方添加下列内容：\n  MX4J_ADDRESS=\u0026quot;-Dmx4jaddress=\u0026lt;Cassandra Node IP\u0026gt;\u0026quot; # e.g. localhost or 127.0.0.1 MX4J_PORT=\u0026quot;-Dmx4jport=\u0026lt;MX4J port\u0026gt;\u0026quot; # default port: 8081 JVM_OPTS=\u0026quot;$JVM_OPTS $MX4J_ADDRESS\u0026quot; JVM_OPTS=\u0026quot;$JVM_OPTS $MX4J_PORT\u0026quot;  重启cassandra服务： sudo service cassandra restart， 随后，可以在cassandra的system log( /var/log/cassandra/system.log )里找到如下信息：  INFO [main] 2016-06-20 10:18:11,493 Mx4jTool.java:63 - mx4j successfully loaded  打开浏览器，输入地址：localhost:8081  ","date":"2021-01-20","permalink":"https://richard-chen93.github.io/post/cassandra%E7%9A%84%E5%AE%89%E8%A3%85%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%E7%9B%91%E6%8E%A7/","tags":["technology"],"title":"Cassandra的安装、集群部署、监控"},{"content":"1、centos7安装kvm 1、前提条件 2. 首先验证CPU是否支持虚拟化，输入有vmx或svm就支持，支持虚拟化则就支持KVM\n[root@openstack ~]# cat /proc/cpuinfo | egrep 'vmx|svm'  3. 查看是否加载KVM\n[root@openstack ~]# lsmod | grep kvm kvm_intel 170086 0 kvm 566340 1 kvm_intel irqbypass 13503 1 kvm  这表明已经加载，如果没有加载则执行以下命令加载KVM\n[root@openstack ~]# modprobe kvm  2、 安装KVM相关软件包 sudo yum install qemu-kvm qemu-img \\ virt-manager libvirt libvirt-python virt-manager \\ libvirt-client virt-install virt-viewer -y  qemu-kvm: KVM模块\nlibvirt: 虚拟管理模块\nvirt-manager: 图形界面管理虚拟机\nvirt-install: 虚拟机命令行安装工具\n启动libvirt并设置开机自启动\n[root@openstack ~]# systemctl start libvirtd [root@openstack ~]# systemctl enable libvirtd  2、常用操作 1、创建虚拟机（命令行） qemu-img create -f qcow2 /kvm_images/vm1.qcow2 10G  virt-install --name=vm1 --vcpus=1 --memory=512 --location=/tmp/CentOS-7-x86_64-Minimal-2009.iso --disk path=/kvm_images/vm1.qcow2,size=10,format=qcow2 --network bridge=virbr0 --graphics none --extra-args='console=ttyS0' --force  2、扩容磁盘   查看磁盘虚拟机详细配置信息，找到虚拟机使用的磁盘镜像位置。\nvirsh edit {vm name}\n  扩容磁盘镜像\nqemu-img\tresize /kvm_images/app5.raw\t+200G    虚拟机里查看扩容后的磁盘大小\nfdisk -l\n若没有变大，宿主机执行 virsh destroy {vm_name}，强关虚拟机，再开起来virsh start {vm_name}\n  虚拟机扩容文件系统\nxfs_grows /dev/vdb    ","date":"2021-01-18","permalink":"https://richard-chen93.github.io/post/kvm%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AE%89%E8%A3%85%E5%92%8C%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/","tags":["technology"],"title":"KVM虚拟机安装和常用操作"},{"content":"CentOS7扩容根分区(LVM+非LVM) \n神冰凰关注\n0.6612018.06.14 14:11:25字数 312阅读 18,450\nCentOS7，LVM根分区扩容步骤： 1.查看现有分区大小\n df -TH\n LVM分区，磁盘总大小为20G,根分区总容量为17G\n2.关机增加大小为30G(测试环境使用的Vmware Workstation)\n扩展分区到30G\n3.查看扩容后磁盘大小\n df -TH\nlsblk\n 磁盘总大小为30G,根分区为17G\n4.创建分区\n fdisk /dev/sda\n 将sda剩余空间全部给sda3\n5.刷新分区并创建物理卷\n partprobe /dev/sda\npvcreate /dev/sda3\n 6.查看卷组名称，以及卷组使用情况\n vgdisplay\n VG Name为centos\n7.将物理卷扩展到卷组\n vgextend centos /dev/sda3\n 使用sda3扩展VG centos\n8.查看当前逻辑卷的空间状态\n lvdisplay\n 需要扩展LV /dev/centos/root\n9.将卷组中的空闲空间扩展到根分区逻辑卷\n lvextend -l +100%FREE /dev/centos/root\n 10.刷新根分区\n xfs_growfs /dev/centos/root\n 11.查看磁盘使用情况，扩展之前和之后是不一样的\n根分区已经变成27G\nCentOS7，非LVM根分区扩容步骤： 1.查看现有的分区大小\n非LVM分区，目前磁盘大小为20G，根分区总容量为17G\n2.关机增加磁盘大小为30G\n3.查看磁盘扩容后状态\n lsblk\ndh -TH\n 现在磁盘总大小为30G,根分区为17G\n4.进行分区扩展磁盘，记住根分区起始位置和结束位置\n5.删除根分区，切记不要保存\n6.创建分区，箭头位置为分区起始位置\n7.保存退出并刷新分区\n partpeobe /dev/sda\n 8.查看分区状态\n这里不知道为啥变成19G了。。\n9.刷新根分区并查看状态\n xfs_growfs /dev/sda3\n 根分区大小已变为27G\n","date":"2021-01-17","permalink":"https://richard-chen93.github.io/post/centos7%E6%89%A9%E5%AE%B9%E6%A0%B9%E5%88%86%E5%8C%BA/","tags":["technology"],"title":"CentOS7扩容根分区"},{"content":"0、利索的脚本： 前提：\n3台机器上都配置好ceph源，并yum makecache。在ceph-deploy上安装ceph-deploy并执行下列脚本：\n#!/bin/bash cd cd mkdir my-cluster cd my-cluster ceph-deploy new s4 s5 s6 export CEPH_DEPLOY_REPO_URL=http://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-luminous/el7 export CEPH_DEPLOY_GPG_URL=http://mirrors.tuna.tsinghua.edu.cn/ceph/keys/release.asc cat \u0026gt;\u0026gt; ./ceph.conf \u0026lt;\u0026lt; EOF public_network = 10.3.3.0/24 cluster_network = 10.3.3.0/24 EOF ceph-deploy install --release luminous s4 s5 s6 ceph-deploy mon create-initial ceph-deploy admin s4 s5 s6 ceph-deploy osd create s4 --data /dev/sdb ceph-deploy osd create s5 --data /dev/sdb ceph-deploy osd create s6 --data /dev/sdb sudo chmod +r /etc/ceph/ceph.client.admin.keyring ceph-deploy mgr create s4 s5 s6 ceph health  一、主机规划    主机名称 系统 IP 配置     s7 CentOS7.6.1810 10.10.10.47 1C1G50G+5G   s8 CentOS7.6.1810 10.10.10.48 1C1G50G+5G   s9 CentOS7.6.1810 10.10.10.49 1C1G50G+5G    磁盘规划\n50G系统盘，5G磁盘为OSD\n二、环境准备 环境准备好，避免踩坑！\n4台机器，s10为ceph-deploy。\ns7 s8 s9为3个node，拥有空闲磁盘sdb，5G\n1、时间同步 s7作为ntp服务器，s8-10作为客户端\nsudo yum -y install ntp ntpdate timedatectl set-timezone Asia/Shanghai  1.s7 节点配置 sudo vim /etc/ntp.conf restrict 10.3.3.7 mask 255.255.255.0 nomodify notrap #配置集群的IP段 server 127.127.1.0 # local clock fudge 127.127.1.0 stratum 10 sudo systemctl enable ntpd sudo systemctl restart ntpd sudo systemctl status ntpd ntpstat ntpq -p date  2.其他节点配置 sudo vim /etc/ntp.conf restrict 10.3.3.7 mask 255.255.255.0 nomodify notrap #IP为node1的ip地址 server 10.3.3.7 # #IP为node1的ip地址 sudo systemctl enable ntpd sudo systemctl restart ntpd sudo systemctl status ntpd ntpstat ntpq -p date  2、ssh免密登录和sudo无需密码权限 s10 需要使用cephuser用户ssh免密登录3个node。\n 4台机器创建cephuser用户。用户名 “ceph” 保留给了 Ceph 守护进程。如果 Ceph 节点上已经有了 “ceph” 用户，升级前必须先删掉这个用户。   sudo useradd cephuser ; echo cephuser | sudo passwd --stdin cephuser   4台机器设定cephuser用户无密码sudo权限  echo \u0026quot;cephuser ALL = (root) NOPASSWD:ALL\u0026quot; | sudo tee /etc/sudoers.d/cephuser sudo chmod 0440 /etc/sudoers.d/cephuser   在S10上使用cephuser用户执行ssh免密登录配置脚本sshops-init.sh \u0026ldquo;s7,s8,s9,s10\u0026rdquo;  3、使用root用户在4台机器上配置ceph 的yum源，安装依赖包   yum源\n把如下内容粘帖进去，保存到 /etc/yum.repos.d/ceph.repo 文件中。\ncat \u0026gt; /etc/yum.repos.d/ceph.repo \u0026lt;\u0026lt;EOF [ceph-luminous-noarch] name = ceph-luminous-noarch baseurl = https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-luminous/el7/noarch/ enabled = 1 gpgcheck = 0 gkgkey = http://mirrors.tuna.tsinghua.edu.cn/ceph/keys/release.asc [ceph-luminous-x64] name = ceph-luminous-x64 baseurl = https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-luminous/el7/x86_64/ enabled = 1 gpgcheck = 0 gkgkey = http://mirrors.tuna.tsinghua.edu.cn/ceph/keys/release.asc EOF wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo yum makecache    安装pip包：\n  wget https://files.pythonhosted.org/packages/5f/ad/1fde06877a8d7d5c9b60eff7de2d452f639916ae1d48f0b8f97bf97e570a/distribute-0.7.3.zip sudo yum -y install unzip unzip distribute-0.7.3.zip cd distribute-0.7.3 python setup.py install cd   安装rpm包：  yum -y install deltarpm  三、Ceph部署 使用ceph-deploy工具部署\n3.1 使用国内源 export CEPH_DEPLOY_REPO_URL=http://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-luminous/el7 export CEPH_DEPLOY_GPG_URL=http://mirrors.tuna.tsinghua.edu.cn/ceph/keys/release.asc  3.2 安装ceph-deploy ceph安装过程中依赖部分epel软件源\nsudo yum -y install epel-release # 使用国内epel源 sudo sed -e 's!^metalink=!#metalink=!g' \\ -e 's!^#baseurl=!baseurl=!g' \\ -e 's!//download\\.fedoraproject\\.org/pub!//mirrors.tuna.tsinghua.edu.cn!g' \\ -e 's!http://mirrors\\.tuna!https://mirrors.tuna!g' \\ -i /etc/yum.repos.d/epel.repo /etc/yum.repos.d/epel-testing.repo sudo yum -y install ceph-deploy  3.3 创建工作目录 mkdir my-cluster cd my-cluster  3.4 创建ceph集群，部署新的monitor节点 ceph-deploy new s4 s5 s6  3.5 修改配置文件 增加public_network和cluster_network配置\ncat \u0026gt;\u0026gt; ./ceph.conf \u0026lt;\u0026lt; EOF public_network = 10.3.3.0/24 cluster_network = 10.3.3.0/24 EOF  3.6 安装Ceph到各个节点 需要指定版本，不指定默认安装最新的版本\nceph-deploy install --release luminous s4 s5 s6  3.7 查看ceph版本 ceph --version ceph version 12.2.13 (584a20eb0237c657dc0567da126be145106aa47e) luminous (stable)  3.8 获取密钥key，会在my-cluster目录下生成几个key ceph-deploy mon create-initial  3.9 分发key ceph-deploy admin s4 s5 s6  3.10 初始化磁盘 ceph-deploy osd create s4 --data /dev/sdb ceph-deploy osd create s5 --data /dev/sdb ceph-deploy osd create s6 --data /dev/sdb  3.11 查看osd设备列表 ceph osd tree ID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF -1 0.58589 root default -3 0.19530 host s7 0 hdd 0.19530 osd.0 up 1.00000 1.00000 -5 0.19530 host s8 1 hdd 0.19530 osd.1 up 1.00000 1.00000 -7 0.19530 host s9 2 hdd 0.19530 osd.2 up 1.00000 1.00000  3.12 给admin key赋权限 sudo chmod +r /etc/ceph/ceph.client.admin.keyring  3.13 创建ceph 管理进程服务 ceph-deploy mgr create s4 s5 s6  3.14 检查健康状况 ceph health HEALTH_OK  四、Ceph块存储 4.1 创建存储池 rados mkpool rbd successfully created pool rbd  4.2 创建块设备 rbd create rbd1 --size 4096  4.3 查看创建的rbd rbd list rbd1 # 查看rbd细节 rbd --image rbd1 info rbd image 'rbd1': size 1GiB in 256 objects order 22 (4MiB objects) block_name_prefix: rbd_data.103d6b8b4567 format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten flags: create_timestamp: Sat Jun 20 21:17:50 2020  4.4 映射到磁盘 rbd feature disable rbd1 object-map fast-diff deep-flatten sudo rbd map rbd/rbd1 /dev/rbd0 5# 格式化磁盘 sudo mkfs.xfs /dev/rbd0 # 挂载 sudo mount /dev/rbd0 /opt  4.5 删除块设备 rbd rm rbd1 Removing image: 100% complete...done.  4.6 删除存储池 默认情况下mon节点不允许删除pool\nrados rmpool rbd rbd --yes-i-really-really-mean-it Check your monitor configuration - `mon allow pool delete` is set to false by default, change it to true to allow deletion of pools # 修改ceph.conf vim /etc/ceph/ceph.conf ... mon_allow_pool_delete = true # 重启ceph-mon.target systemctl restart ceph-mon.target  再次删除pool\nrados rmpool rbd rbd --yes-i-really-really-mean-it successfully deleted pool rbd  五、Ceph对象存储 5.1 创建对象存储网关 1ceph-deploy rgw create s7 s8 s9  创建完成之后默认监听7480端口。然后可以使用负载均衡的方式转发到后端服务。\n5.2 创建s3用户 radosgw-admin user create --uid=admin --display-name=admin --email=admin@example.com { \u0026quot;user_id\u0026quot;: \u0026quot;admin\u0026quot;, \u0026quot;display_name\u0026quot;: \u0026quot;admin\u0026quot;, \u0026quot;email\u0026quot;: \u0026quot;admin@example.com\u0026quot;, \u0026quot;suspended\u0026quot;: 0, \u0026quot;max_buckets\u0026quot;: 1000, \u0026quot;auid\u0026quot;: 0, \u0026quot;subusers\u0026quot;: [], \u0026quot;keys\u0026quot;: [ { \u0026quot;user\u0026quot;: \u0026quot;admin\u0026quot;, \u0026quot;access_key\u0026quot;: \u0026quot;837H72BJ7KJ4ZO7Q7PJL\u0026quot;, \u0026quot;secret_key\u0026quot;: \u0026quot;GYgDMcqxFI68A5K10sWlA2GF9cknohFPqUb6499b\u0026quot; } ], \u0026quot;swift_keys\u0026quot;: [], \u0026quot;caps\u0026quot;: [], \u0026quot;op_mask\u0026quot;: \u0026quot;read, write, delete\u0026quot;, \u0026quot;default_placement\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;placement_tags\u0026quot;: [], \u0026quot;bucket_quota\u0026quot;: { \u0026quot;enabled\u0026quot;: false, \u0026quot;check_on_raw\u0026quot;: false, \u0026quot;max_size\u0026quot;: -1, \u0026quot;max_size_kb\u0026quot;: 0, \u0026quot;max_objects\u0026quot;: -1 }, \u0026quot;user_quota\u0026quot;: { \u0026quot;enabled\u0026quot;: false, \u0026quot;check_on_raw\u0026quot;: false, \u0026quot;max_size\u0026quot;: -1, \u0026quot;max_size_kb\u0026quot;: 0, \u0026quot;max_objects\u0026quot;: -1 }, \u0026quot;temp_url_keys\u0026quot;: [], \u0026quot;type\u0026quot;: \u0026quot;rgw\u0026quot; }  记住用户的 access_key和secret_key，后面需要用到这些信息用于访问s3服务。\n5.3 删除用户 1radosgw-admin user rm --uid=admin  5.4 使用s3cmd客户端访问 安装s3cmd\n1yum -y install s3cmd  配置s3客户端\ns3cmd --configure \\ --access_key=837H72BJ7KJ4ZO7Q7PJL \\ --secret_key=GYgDMcqxFI68A5K10sWlA2GF9cknohFPqUb6499b \\ --host=10.10.10.47:7480 \\ --host-bucket=test-bucket \\ --no-ssl Enter new values or accept defaults in brackets with Enter. Refer to user manual for detailed description of all options. Access key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variables. Access Key [837H72BJ7KJ4ZO7Q7PJL]: Secret Key [GYgDMcqxFI68A5K10sWlA2GF9cknohFPqUb6499b]: Default Region [US]: Use \u0026quot;s3.amazonaws.com\u0026quot; for S3 Endpoint and not modify it to the target Amazon S3. S3 Endpoint [10.10.10.47:7480]: Use \u0026quot;%(bucket)s.s3.amazonaws.com\u0026quot; to the target Amazon S3. \u0026quot;%(bucket)s\u0026quot; and \u0026quot;%(location)s\u0026quot; vars can be used if the target S3 system supports dns based buckets. DNS-style bucket+hostname:port template for accessing a bucket [test-bucket]: Encryption password is used to protect your files from reading by unauthorized persons while in transfer to S3 Encryption password: Path to GPG program [/usr/bin/gpg]: When using secure HTTPS protocol all communication with Amazon S3 servers is protected from 3rd party eavesdropping. This method is slower than plain HTTP, and can only be proxied with Python 2.7 or newer Use HTTPS protocol [No]: On some networks all internet access must go through a HTTP proxy. Try setting it here if you can't connect to S3 directly HTTP Proxy server name: New settings: Access Key: 837H72BJ7KJ4ZO7Q7PJL Secret Key: GYgDMcqxFI68A5K10sWlA2GF9cknohFPqUb6499b Default Region: US S3 Endpoint: 10.10.10.47:7480 DNS-style bucket+hostname:port template for accessing a bucket: test-bucket Encryption password: Path to GPG program: /usr/bin/gpg Use HTTPS protocol: False HTTP Proxy server name: HTTP Proxy server port: 0 Test access with supplied credentials? [Y/n] y Please wait, attempting to list all buckets... Success. Your access key and secret key worked fine :-) Now verifying that encryption works... Not configured. Never mind. Save settings? [y/N] y Configuration saved to '/root/.s3cfg'  创建bucket\ns3cmd mb s3://test-bucket Bucket 's3://test-bucket/' created  查看bucket\ns3cmd ls 2020-06-20 14:54 s3://test-bucket  上传文件到bucket\ns3cmd put ceph.conf s3://test-bucket upload: 'ceph.conf' -\u0026gt; 's3://test-bucket/ceph.conf' [1 of 1] 340 of 340 100% in 2s 163.23 B/s done  查看bucket中的文件\ns3cmd ls s3://test-bucket 2020-06-20 14:55 340 s3://test-bucket/ceph.conf  更多s3cmd的操作可使用s3cmd -h查看或者通过访问s3cmd官网查看。\n六、Ceph-Dashboard Ceph 的监控可视化界面方案很多—-grafana、Kraken。但是从Luminous开始，Ceph 提供了原生的Dashboard功能，通过Dashboard可以获取Ceph集群的各种基本状态信息。\n6.1 配置Dashboard # 开启mgr功能 ceph mgr module enable dashboard # 生成并安装自签名的证书 ceph dashboard create-self-signed-cert # 创建一个dashboard登录用户名密码 ceph dashboard ac-user-create guest 1q2w3e4r administrator  6.2 修改默认配置 # 指定集群dashboard的访问端口 ceph config-key set mgr/dashboard/server_port 7000 # 指定集群 dashboard的访问IP ceph config-key set mgr/dashboard/server_addr $IP  6.3 开启Object Gateway管理功能 # 创建rgw用户 radosgw-admin user info --uid=admin # 提供Dashboard证书 ceph dashboard set-rgw-api-access-key $access_key ceph dashboard set-rgw-api-secret-key $secret_key # 配置rgw主机名和端口 ceph dashboard set-rgw-api-host 192.168.0.251  了解更多/usr/lib64/ceph/mgr/dashboard/README.rst\n 原文作者：黄忠德 **原文链接：**https://huangzhongde.cn/post/Linux/Ceph_luminous_deploy/ **版权声明：**本作品采用知识共享署名-非商业性使用-禁止演绎 4.0 国际许可协议进行许可，非商业转载请注明出处（作者，原文链接），商业转载请联系作者获得授权。  See Also  Ceph-10.2.10安装配置 Redis高可用之redis-cluster集群 Redis高可用之哨兵模式 使用HAProxy+Pacemaker部署RabbitMQ高可用集群 MariaDB HA之Galera集群  ","date":"2021-01-14","permalink":"https://richard-chen93.github.io/post/ceph_12_luminous%E5%AE%89%E8%A3%85/","tags":["technology"],"title":"Ceph_12_luminous安装"},{"content":"1、安装httpd memcached php php-memcache sudo yum -y install httpd memcached php php-memcache  2、配置php-memcache扩展 Note： 是memcache，而不是memcached\nvim /etc/php.ini #末尾添加如下配置 extension_dir = “/usr/lib64/php/modules”  3、启动httpd、memcached memcached -d -u root -l 127.0.0.1 -p 11211 -m 128 {PID_file} #提前创建该pid文件 sudo systemctl start httpd echo \u0026quot;\u0026lt;?php phpinfo(); ?\u0026gt;\u0026quot; \u0026gt; /var/www/html/index.php #测试php是否正常 memcached-tool 127.0.0.1:11211 stats #查看memcached状态，是否已启动 cat /etc/sysconfig/memcached\t#查看memcached配置信息  4、下载安装memadmin  下载地址 http://www.junopen.com/memadmin 将压缩包解压到/var/www/html 此文件可设定用户名密码 config.php 浏览器打开监控 : http://example.com/memadmin/index.php  ","date":"2021-01-12","permalink":"https://richard-chen93.github.io/post/%E5%AE%89%E8%A3%85%E5%B9%B6%E7%9B%91%E6%8E%A7memcached/","tags":["technology"],"title":"安装并监控memcached"},{"content":"安装完初始化： mysql_secure_installation  创建用户 CREATE USER 'panda'@'localhost' IDENTIFIED BY 'panda';  允许其他IP访问数据库 %代表任意IP GRANT ALL PRIVILEGES ON *.* TO 'panda'@'%' IDENTIFIED BY 'panda' WITH GRANT OPTION; flush privileges;  创建数据库表 CREATE TABLE IF NOT EXISTS `runoob_tbl`( `runoob_id` INT UNSIGNED AUTO_INCREMENT, `runoob_title` VARCHAR(100) NOT NULL, `runoob_author` VARCHAR(40) NOT NULL, `submission_date` DATE, PRIMARY KEY ( `runoob_id` ) )ENGINE=InnoDB DEFAULT CHARSET=utf8;  修改root密码 使用自动生成的密码登录mysql以后\n修改密码mysql\u0026gt; alter user \u0026lsquo;root\u0026rsquo;@\u0026lsquo;localhost\u0026rsquo; identified by \u0026lsquo;you_new_password\u0026rsquo;;\n忘记root密码 vim /etc/my.cnf\n[mysqld] skip-grant-tables #添加这一行  service mysqld restart  mysql -h 127.0.0.1 -uroot use mysql; flush privileges; alter user 'root'@'localhost' identified by '123456'; flush privileges; quit  # my.cnf配置文件去掉skip-grant-tables 这一行，重启mysql  ","date":"2021-01-12","permalink":"https://richard-chen93.github.io/post/mysql%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/","tags":["technology"],"title":"Mysql常用操作"},{"content":"1、环境准备 环境准备好，避免踩坑！\n4台机器，s10为ceph-deploy。\ns7 s8 s9为3个node，拥有空闲磁盘sdb，10G\nceph-deploy \u0026ndash;version 1.5.39\nceph version 10.2.11\n1、时间同步 s7作为ntp服务器，s8-10作为客户端\nsudo yum -y install ntp ntpdate  1.s7 节点配置 sudo vim /etc/ntp.conf restrict 10.3.3.7 mask 255.255.255.0 nomodify notrap #配置集群的IP段 server 127.127.1.0 # local clock fudge 127.127.1.0 stratum 10 sudo systemctl enable ntpd sudo systemctl restart ntpd sudo systemctl status ntpd ntpstat ntpq -p date  2.其他节点配置 sudo vim /etc/ntp.conf restrict 10.3.3.7 mask 255.255.255.0 nomodify notrap #IP为node1的ip地址 server 10.3.3.7 # #IP为node1的ip地址 sudo systemctl enable ntpd sudo systemctl restart ntpd sudo systemctl status ntpd ntpstat ntpq -p date  2、ssh免密登录和sudo无需密码权限 s10 需要使用cephuser用户ssh免密登录3个node。\n 4台机器创建cephuser用户。用户名 “ceph” 保留给了 Ceph 守护进程。如果 Ceph 节点上已经有了 “ceph” 用户，升级前必须先删掉这个用户。   sudo useradd cephuser ; echo cephuser | sudo passwd --stdin cephuser   4台机器设定cephuser用户无密码sudo权限  echo \u0026quot;cephuser ALL = (root) NOPASSWD:ALL\u0026quot; | sudo tee /etc/sudoers.d/cephuser sudo chmod 0440 /etc/sudoers.d/cephuser   在S10上使用cephuser用户执行ssh免密登录配置脚本sshops-init.sh \u0026ldquo;s7,s8,s9,s10\u0026rdquo;  3、在4台机器上配置ceph 的yum源，安装依赖包   yum源\nsudo vi /etc/yum.repos.d/ceph.repo  把如下内容粘帖进去，保存到 /etc/yum.repos.d/ceph.repo 文件中。\n[ceph] name=ceph baseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/x86_64/ gpgcheck=0 priority=1 [ceph-noarch] name=cephnoarch baseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/noarch/ gpgcheck=0 priority=1 [ceph-source] name=Ceph source packages baseurl=http://mirrors.aliyun.com/ceph/rpm-jewel/el7/SRPMS gpgcheck=0 priority=1    安装pip包：\n  wget https://files.pythonhosted.org/packages/5f/ad/1fde06877a8d7d5c9b60eff7de2d452f639916ae1d48f0b8f97bf97e570a/distribute-0.7.3.zip sudo yum -y install unzip unzip distribute-0.7.3.zip cd distribute-0.7.3 sudo python setup.py install   安装rpm包：  sudo yum -y install deltarpm  排错参考：http://www.voidcn.com/article/p-rougfoll-ov.html\n集群搭建：http://docs.ceph.org.cn/start/quick-ceph-deploy/\n4、s10安装ceph-deploy 安装ceph-deploy\nsudo yum -y install ceph-deploy  查看ceph-deploy是否安装成功\nceph-deploy --version  2、搭建集群 1、说明 第一次练习时，我们创建一个 Ceph 存储集群，它有一个 Monitor 和两个 OSD 守护进程。一旦集群达到 active + clean 状态，再扩展它：增加第三个 OSD 、增加元数据服务器和两个 Ceph Monitors。为获得最佳体验，先在管理节点上创建一个目录，用于保存 ceph-deploy 生成的配置文件和密钥对。\nmkdir my-cluster cd my-cluster  注意： ceph-deploy 会把文件输出到当前目录，所以请确保在此目录下执行 ceph-deploy 。\nImportant:\n如果你是用另一普通用户登录的，不要用 sudo 或在 root 身份运行 ceph-deploy ，因为它不会在远程主机上调用所需的 sudo 命令。\n如果在某些地方碰到麻烦，想从头再来，可以用下列命令清除配置：\nceph-deploy purgedata {ceph-node} [{ceph-node}] ceph-deploy forgetkeys  用下列命令可以连 Ceph 安装包一起清除：\nceph-deploy purge {ceph-node} [{ceph-node}]  如果执行了 purge ，你必须重新安装 Ceph 。\n2、创建ceph集群 Note： 在管理节点上，进入刚创建的放置配置文件的目录，用 ceph-deploy 执行如下步骤。\n1、创建集群。\nceph-deploy new s7  在当前目录下用 ls 和 cat 检查 ceph-deploy 的输出，应该有一个 Ceph 配置文件、一个 monitor 密钥环和一个日志文件。详情见 ceph-deploy new -h 。\n2、把 Ceph 配置文件里的默认副本数从 3 改成 2 ，这样只有两个 OSD 也可以达到 active + clean 状态。把下面这行加入 [global] 段：\n sed -i '$a\\osd pool default size = 2' ceph.conf  3、如果你有多个网卡，可以把 public network 写入 Ceph 配置文件的 [global] 段下。详情见网络配置参考。\npublic network = {ip-address}/{netmask}  4、安装 Ceph 。\nceph-deploy install s7 s8 s9 s10  ceph-deploy 将在各节点安装 Ceph 。 **注：**如果你执行过 ceph-deploy purge ，你必须重新执行这一步来安装 Ceph 。\n5、配置初始 monitor(s)、并收集所有密钥：\nceph-deploy mon create-initial  完成上述操作后，当前目录里应该会出现这些密钥环：\n {cluster-name}.client.admin.keyring {cluster-name}.bootstrap-osd.keyring {cluster-name}.bootstrap-mds.keyring {cluster-name}.bootstrap-rgw.keyring  3、增加/删除osd 创建 OSD 你可以用 create 命令一次完成准备 OSD 、部署到 OSD 节点、并激活它。 create 命令是依次执行 prepare 和 activate 命令的捷径。\n如果创建osd失败，怎么弄都是失败，那就将各node sdb都分区为sdb1,xfs格式，并且创建osd时使用s7:/dev/sdb1， 而不是/dev/sdb。 因为没有时间和精力为了一个别人的BUG而执着于此，切记！\nceph-deploy osd create s7:/dev/sdb s8:/dev/sdb #在两个节点创建osd ceph-deploy osd create osdserver1:sdb:/dev/ssd1 #数据放sdb，日志放ssd盘  把配置文件和 admin 密钥拷贝到管理节点和 Ceph 节点  ceph-deploy admin s7 s8 s9 s10  ceph mds stat 查看MDS\n确保ceph.client.admin.keyring的权限正确 sudo chmod +r /etc/ceph/ceph.client.admin.keyring  安装 radosgw ceph-deploy rgw create s7  检查集群的健康状况和OSD节点状况 ceph health ceph -s cluster 3756d8ae-6f85-40f0-abd3-2a2863ec37dc health HEALTH_OK monmap e1: 1 mons at {node1=192.168.6.150:6789/0} election epoch 3, quorum 0 node1 osdmap e20: 2 osds: 2 up, 2 in flags sortbitwise,require_jewel_osds pgmap v44: 112 pgs, 7 pools, 1588 bytes data, 171 objects 225 MB used, 3563 GB / 3563 GB avail 112 active+clean client io 50376 B/s rd, 0 B/s wr, 49 op/s rd, 32 op/s wr  查看是否安装成功\ncurl http://s7:7480 \u0026lt;ListAllMyBucketsResult xmlns=\u0026quot;http://s3.amazonaws.com/doc/2006-03-01/\u0026quot;\u0026gt; \u0026lt;Owner\u0026gt; \u0026lt;ID\u0026gt;anonymous\u0026lt;/ID\u0026gt; \u0026lt;DisplayName/\u0026gt; \u0026lt;/Owner\u0026gt; \u0026lt;Buckets/\u0026gt; \u0026lt;/ListAllMyBucketsResult\u0026gt;  4、ceph文件系统 Ceph 文件系统要求 Ceph 存储集群内至少有一个 Ceph 元数据服务器。\n1、增加一元数据服务器 部署完监视器和 OSD 后，还可以部署元数据服务器。\nceph-deploy mds create s7:{daemon-name}] [{host-name}[:{daemon-name}] ...]  如果你想在同一主机上运行多个守护进程，可以为每个进程指定名字（可选）。\n2、拆除一元数据服务器 尚未实现……？\n3、创建文件系统 一个 Ceph 文件系统需要至少两个 RADOS 存储池，一个用于数据、一个用于元数据。配置这些存储池时需考虑：\n 为元数据存储池设置较高的副本水平，因为此存储池丢失任何数据都会导致整个文件系统失效。 为元数据存储池分配低延时存储器（像 SSD ），因为它会直接影响到客户端的操作延时。  关于存储池的管理请参考 存储池 。例如，要用默认设置为文件系统创建两个存储池，你可以用下列命令：\n$ ceph osd pool create cephfs_data \u0026lt;pg_num\u0026gt; $ ceph osd pool create cephfs_metadata \u0026lt;pg_num\u0026gt;  创建好存储池后，你就可以用 fs new 命令创建文件系统了：\n$ ceph fs new \u0026lt;fs_name\u0026gt; \u0026lt;metadata\u0026gt; \u0026lt;data\u0026gt;  例如：\n$ ceph fs new cephfs cephfs_metadata cephfs_data $ ceph fs ls name: cephfs, metadata pool: cephfs_metadata, data pools: [cephfs_data ]  文件系统创建完毕后， MDS 服务器就能达到 active 状态了，比如在一个单 MDS 系统中：\n$ ceph mds stat e5: 1/1/1 up {0=a=up:active}  建好文件系统且 MDS 活跃后，你就可以挂载此文件系统了：\n要挂载 Ceph 文件系统，如果你知道监视器 IP 地址可以用 mount 命令、或者用 mount.ceph 工具来自动解析监视器 IP 地址。例如：\nsudo mkdir /mnt/mycephfs sudo mount -t ceph 192.168.0.1:6789:/ /mnt/mycephfs  要挂载启用了 cephx 认证的 Ceph 文件系统，你必须指定用户名、密钥。\nsudo mount -t ceph 192.168.0.1:6789:/ /mnt/mycephfs -o name=admin,secret=AQATSKdNGBnwLhAAnNDKnH65FmVKpXZJVasUeQ==  前述用法会把密码遗留在 Bash 历史里，更安全的方法是从文件读密码。例如：\nsudo mount -t ceph 192.168.0.1:6789:/ /mnt/mycephfs -o name=admin,secretfile=/etc/ceph/admin.secret  关于 cephx 参见认证。\n要卸载 Ceph 文件系统，可以用 unmount 命令，例如：\nsudo umount /mnt/mycephfs  其他： 我的thinkpad，克隆虚拟机后的操作：\nsudo sed -i \u0026quot;s/1f76218ec253/1f76218ec222/g\u0026quot; /etc/sysconfig/network-scripts/ifcfg-ens33 sudo sed -i \u0026quot;s/IPADDR=10.3.3.253/IPADDR=10.3.3.222/g\u0026quot; /etc/sysconfig/network-scripts/ifcfg-ens33 sudo systemctl restart network cd  ","date":"2021-01-12","permalink":"https://richard-chen93.github.io/post/ceph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/","tags":["technology"],"title":"Ceph集群部署"},{"content":"安装NPT（所有节点） 配置了ntp启动正常，设置开机自启，发现重启后启动不起来，且无任何报错？ 坑爹！disable 和stop chronyd !\n我们建议在所有 Ceph 节点上安装 NTP 服务，以免因时钟漂移导致故障。\nsudo yum install ntp ntpdate  1.node1节点配置 node1节点作为ntp服务器\nsudo vim /etc/ntp.conf\t#删除所有默认的restrict 和 server配置，添加以下内容：  restrict 10.3.3.4 mask 255.255.255.0 nomodify notrap #配置集群的IP段 server 127.127.1.0 # local clock fudge 127.127.1.0 stratum 10  sudo systemctl enable ntpd sudo systemctl restart ntpd systemctl status ntpd  2.其他节点配置 sudo vim /etc/ntp.conf  restrict 10.3.3.4 mask 255.255.255.0 nomodify notrap #IP为node1的ip地址 server 10.3.3.4 # #IP为node1的ip地址  sudo systemctl enable ntpd sudo systemctl restart ntpd systemctl status ntpd  3.查看时间同步状态 $ ntpstat #这里显示的是与local本地同步的，代表还没有和外网服务器进行时间同步 #synchronised to local net (127.127.1.0) at stratum 11 # time correct to within 11 ms # polling server every 64 s #输出上述内容代表同步成功  ","date":"2021-01-03","permalink":"https://richard-chen93.github.io/post/%E5%86%85%E7%BD%91ntp%E6%97%B6%E9%97%B4%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%90%AD%E5%BB%BA/","tags":["technology"],"title":"内网ntp时间服务器搭建"},{"content":"查看cpu信息 [root@AAA ~]# cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c 24 Intel(R) Xeon(R) CPU E5-2630 0 @ 2.30GHz # 查看物理CPU个数 [root@AAA ~]# cat /proc/cpuinfo| grep \u0026quot;physical id\u0026quot;| sort| uniq| wc -l 2 # 查看每个物理CPU中core的个数(即核数) [root@AAA ~]# cat /proc/cpuinfo| grep \u0026quot;cpu cores\u0026quot;| uniq cpu cores : 6 # 查看逻辑CPU的个数 [root@AAA ~]# cat /proc/cpuinfo| grep \u0026quot;processor\u0026quot;| wc -l 24  文本编辑 cat \u0026gt;\u0026gt; /etc/my.cnf \u0026lt;\u0026lt; EOF text 1 text 2 EOF  sed -i 's/原字符串/新字符串/' /home/1.txt  sudo useradd cephuser ; echo cephuser | sudo passwd \u0026ndash;stdin cephuser\n我的macbook15，windows下的虚拟机：\nsed -i 's/920f1da23253/920f1da23011/' /etc/sysconfig/network-scripts/ifcfg-ens33 sed -i 's/IPADDR=10.0.3.253/IPADDR=10.0.3.11/' /etc/sysconfig/network-scripts/ifcfg-ens33 systemctl restart network  ","date":"2020-12-25","permalink":"https://richard-chen93.github.io/post/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","tags":["technology"],"title":"Linux常用命令"},{"content":"OS和mysql版本 mysql5.7、centos7\n报错记录 mysql启动失败，报错 The server quit without updating PID file！ 修改启动脚本 /etc/init.d/mysqld 约278行，增加\u0026ndash;user=root参数 $bindir/mysqld_safe \u0026ndash;user=root \u0026ndash;datadir=\u0026quot;$datadir\u0026quot; \u0026ndash;pid-file=\u0026quot;$mysqld_pid_file_path\u0026quot; $other_args \u0026gt;/dev/null \u0026amp;\n确保/etc/下有my.cnf.d\n修改root密码 使用自动生成的密码登录mysql以后\n修改密码mysql\u0026gt; alter user \u0026lsquo;root\u0026rsquo;@\u0026lsquo;localhost\u0026rsquo; identified by \u0026lsquo;you_new_password\u0026rsquo;;\nmysql主从同步集群搭建 1、两台装好mysql后修改配置文件 假设已经安装 A、B两台机器mysql，其中A作为mster，B作为slave。\n  两台机器my.cnf配置文件中server_id 的值不可相同，如A机器为：server_id = 1，B机器设置为 2\n  分别修改A/B两台机器/etc/my.cnf文件一项配置，并重启 mysql (service mysql restart)\n  #A机器 /etc/my.cnf: log-bin=master-bin #B机器 /etc/my.cnf: log-bin= relay-bin  2、A执行： mysql -h 127.0.0.1 -u root -p #登录mysql  在mysql客户端执行命令：创建用户(sync/123456)来给slave同步使用\nmysql\u0026gt; grant replication slave on *.* to 'sync'@'slave机器IP' identified by '123456';  查看master状态，输出如下\nmysql\u0026gt; show master status; +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+ | master-bin.000004 | 229548120 | | | | +\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;-+\nmysql\u0026gt; flush privileges; #刷新配置 service mysqld restart; # 重启mysql  3、B执行： mysql -h 127.0.0.1 -u root -p #登录mysql  mysql\u0026gt; CHANGE MASTER TO -\u0026gt; MASTER_HOST='192.168.6.50', #master所在服务器的IP -\u0026gt; MASTER_USER='sync', #master授权的账号,此处应为sync' -\u0026gt; MASTER_PASSWORD='123456', # master授权的密码，此处应为123456 -\u0026gt; MASTER_LOG_FILE='master-bin.000002', #master的日志文件名master的show master status的file -\u0026gt; MASTER_LOG_POS=0 # master的日志所在位置master的show master status的Position; 注意这一行没有单引号，数值类型，也可以填写为0，由mysql自己获取具体的值 -\u0026gt; ;  Query OK, 0 rows affected, 2 warnings (0.00 sec)\nmysql\u0026gt; start slave; # 开启复制  4、 验证，查看状态 重新登录mysql slave机器，查看其状态：\nmysql\u0026gt; show slave status\\G  Slave_IO_State: Waiting for master to send event Master_Host: master的IP地址 Master_User: root Master_Port: 3306 Connect_Retry: 60 Master_Log_File: master-bin.000001 Read_Master_Log_Pos: 1516 Relay_Log_File: slave-bin.000004 Relay_Log_Pos: 1117 Relay_Master_Log_File: master-bin.000001 Slave_IO_Running: Yes Slave_SQL_Running: Yes \u0026hellip;\u0026hellip;\nSlaveIORunning: YES 表示slave的日志读取线程开启\nSlaveSQLRunning: YES 表示SQL执行线程开启\n两者都为YES表示主从模式成功。\n","date":"2020-12-07","permalink":"https://richard-chen93.github.io/post/mysql%E6%95%85%E9%9A%9C%E8%AE%B0%E5%BD%95/","tags":["technology"],"title":"Mysql故障记录"},{"content":"一、实验背景 1. 内网环境下，无法连接互联网，需要搭建ceph，为分布式集群提供ceph文件系统\n2. 要实现脚本的自动化安装，shell脚本或者ansible playbook，不使用ceph-deploy工具\n我们需要在一台能联网的实验机机器上，将ceph集群安装所需的主包及其依赖一次性下载，编写安装脚本，然后在目标机器上搭建本地yum源，实现离线安装。\n我们先实现搭建本地仓库，在目标机器上手动安装。\n二、实验环境 操作系统：CentOS7.5 Minimal 联网的实验机： 192.168.1.101 cephServer(node01): 192.168.1.103 **cephServer(node01)数据盘：/dev/sdb 100G** cephClient： 192.168.1.106  操作系统：CentOS7.6 desktop 联网的实验机： 10.3.3.39 cephServer(node01): 10.3.3.39 **cephServer(node01)数据盘：/dev/sdb 100G** cephClient： 10.3.3.39  关闭selinux # setenforce 0 # sed -i 's/^SELINUX=.*/SELINUX=permissive/g' /etc/selinux/config 设置防火墙，放行相关端口 # systemctl start firewalld # systemctl enable firewalld # firewall-cmd --zone=public --add-port=6789/tcp --permanent # firewall-cmd --zone=public --add-port=6800-7300/tcp --permanent # firewall-cmd --reload  三、在联网的实验机下载ceph主包及其依赖 添加ceph官方yum镜像仓库\nvi /etc/yum.repos.d/ceph.repo  [Ceph] name=Ceph packages for $basearch baseurl=http://mirrors.163.com/ceph/rpm-luminous/el7/$basearch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc priority=1 [Ceph-noarch] name=Ceph noarch packages baseurl=http://mirrors.163.com/ceph/rpm-luminous/el7/noarch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc priority=1 [ceph-source] name=Ceph source packages baseurl=http://mirrors.163.com/ceph/rpm-luminous/el7/SRPMS enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc  将按照包下载到cephDeps 目录下，生成tar.gz包\nyum clean all yum repolist yum list all |grep ceph yum -y install epel-release yum -y install yum-utils yum -y install createrepo mkdir /root/cephDeps repotrack ceph ceph-mgr ceph-mon ceph-mds ceph-osd ceph-fuse ceph-radosgw -p /root/cephDeps createrepo -v /root/cephDeps tar -zcf cephDeps.tar.gz /root/cephDeps  四、在cephServer(node01)上搭建 本地yum源 将cephDeps.tar.gz拷贝到cephServer(node01)服务器\ntar -zxf cephDeps.tar.gz  vim build_localrepo.sh ################################################## #!/bin/bash parent_path=$( cd \u0026quot;$(dirname \u0026quot;${BASH_SOURCE}\u0026quot;)\u0026quot; ; pwd -P ) cd \u0026quot;$parent_path\u0026quot; mkdir /etc/yum.repos.d/backup mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/backup #create local repositry rm -rf /tmp/localrepo mkdir -p /tmp/localrepo cp -rf ./cephDeps/* /tmp/localrepo echo \u0026quot; [localrepo] name=Local Repository baseurl=file:///tmp/localrepo gpgcheck=0 enabled=1\u0026quot; \u0026gt; /etc/yum.repos.d/ceph.repo yum clean all ################################################## sh -x build_localrepo.sh yum repolist  用本地yum源安装ceph组件\nyum -y install ceph ceph-mds ceph-mgr ceph-osd ceph-mon yum list installed | grep ceph ll /etc/ceph/ ll /var/lib/ceph/  配置ceph组件\n这里未配置： 创建集群id** uidgen 用uidgen 生成一个uuid 例如 ee741368-4233-4cbc-8607-5d36ab314dab  grep -i uuid /etc/sysconfig/network-scripts/ifcfg-ens33 UUID=99cf6cf1-1646-4dbd-bb74-71db9c1dc139  创建ceph主配置文件\n# vim /etc/ceph/ceph.conf\n######################################\n[global] fsid = 99cf6cf1-1646-4dbd-bb74-71db9c1dc139 mon_initial_members = node01 mon_host = 10.3.3.39 mon_max_pg_per_osd = 300 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx osd_pool_default_size = 1 osd_pool_default_min_size = 1 osd_journal_size = 1024 osd_crush_chooseleaf_type = 0 public_network = 10.3.3.0/24 cluster_network = 10.3.3.0/24 [mon] mon allow pool delete = true  ###################################\n","date":"2020-12-01","permalink":"https://richard-chen93.github.io/post/centos7%E5%8D%95%E6%9C%BA%E7%A6%BB%E7%BA%BF%E9%83%A8%E7%BD%B2ceph%E5%8F%8Acephfs%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%BD%BF%E7%94%A8/","tags":["technology"],"title":"Centos7单机离线部署ceph及cephFS文件系统使用"},{"content":"1、kafka架构图及下载 https://archive.apache.org/dist/kafka/0.11.0.0/kafka_2.11-0.11.0.0.tgz\n此外，java安装并配置好环境变量。\n2、修改kafka配置文件 vim ./config/server.properties broker.id=0 #每个broker的唯一标识，不可重复 delete.topic.enable=true #允许删除topic log.dirs=/root/kafka/logs #出于安全考虑，修改默认log位置。kafka的消息队列数据和日志都存储在logs文件夹下面。 （logs文件夹在kafka根目录下创建） #在zookeeper的数据机构中，每个子目录项如 NameService 都被称作为 znode(目录节点)，和文件系统一样，我们能够自由的增加、删除#znode，在一个znode下增加、删除子znode，唯一的不同在于znode是可以存储数据的 zookeeper.connect=s4:2181,s5:2181,s6:2181 #指定zookeeper集群  3、kafka和ZK集群配置文件同步 xync 分发同步 kafka和zk目录 （会经常用到同步工具xsync）\n或使用mobaxterm的multi-exec功能同时修改多台机器的配置。\n4、zookeeper的分布式安装配置 4.1安装 kafka依赖于zookeeper，先下载安装好zookeeper。（三台机器都要安装配置，可以使用同步脚本xsync，或者mobaxterm的的multi-execution功能） 安装好后将zookeeper配置文件模板改为配置文件\nmv zoo_sample.cfg zoo.cfg vim zoo.cfg dataDir=/tmp/zookeeper #修改默认路径，指定路径为/root/zookeeper/zkData  4.2修改配置文件 在zkData目录下创建myid文件，myid文件内容（整型数字：1，2，3）对应三个集群节点的编号。 并在zoo.cfg文件末尾增加如下配置：\nserver.1=s4:2888:3888 server.2=s5:2888:3888 server.3=s6:2888:3888  数字123对应myid文件的内容，指定了节点的编号。 s4,s5,s6是主机名或IP；2888是follower与leader服务器通讯传递副本（replicator）的端口；3888是leader挂掉后集群重新选举时通信的端口。\n4.3启动验证 配置完毕启动zookeeper。查看状态，显示Mode为leader或follower即表示集群启动成功。\n设置开机启动，使用admin用户启动zookeeper：\n$ sudo su #切换到root用户 $ vim /etc/rc.d/rc.local 新增配置 su admin -c \u0026quot;/usr/local/zookeeper/startBase.sh\u0026quot; #组件脚本全路径  startBase.sh脚本内容：\n#!/bin/bash # chkconfig: 2345 60 20 # description: zookeeper start APP_HOME=/usr/local/zookeeper/bin cd $APP_HOME echo $PWD source /etc/profile ./zkServer.sh start ../conf/zoo.cfg  5、启动kafka服务并测试  110 cd kafka/bin/ 106 sh bin/kafka-server-start.sh config/server.properties 113 ./kafka-topics.sh --create --zookeeper s4:2181 --partitions 2 --replication-factor 2 --topic topic01 117 ./kafka-topics.sh --list --zookeeper s4:2181 118 ./kafka-topics.sh --list --zookeeper s5:2181 119 ./kafka-topics.sh --list --zookeeper s6:2181 #描述topic信息 ./kafka-topics.sh --describe --zookeeper s5:2181 --topic topic01 # 启动 kafka，指定配置文件 # 创建topic，指定zk服务器，分区数、副本数、topic名字  6、 生成、消费数据 s4生产数据： kafka-console-producer.sh --broker-list s4:9092 --topic topic01   hi there i am richard\n s6或者s5 消费数据： kafka-console-consumer.sh --zookeeper s5:2181 --topic topic01 #消费最新数据  消费者消费消息,指定消费组名 ./kafka-console-consumer.sh --bootstrap-server s4:9092,s5:9092,s6:9092 --new-consumer --consumer-property group.id=consumer_group01 --topic topic01 #这里可看到所有消费的消息 kjh iouioy khkhjkl lllllllllllllllllll  查看正在运行的消费组 ./kafka-consumer-groups.sh --bootstrap-server s4:9092 --list --new-consumer Note: This will only show information about consumers that use the Java consumer API (non-ZooKeeper-based consumers). consumer_group01  消费堆积情况查看 ./kafka-consumer-groups.sh --bootstrap-server s4:9092 --describe --group consumer_group01 Note: This will only show information about consumers that use the Java consumer API (non-ZooKeeper-based consumers). TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID topic01 0 1 1 0 consumer-1-dd3fc747-7a23-4057-a5e3-e6268f2ee802 /10.3.3.5 consumer-1 topic01 1 1 1 0 consumer-1-dd3fc747-7a23-4057-a5e3-e6268f2ee802 /10.3.3.5 consumer-1  消费所有数据，从头开始 bin/kafka-console-consumer.sh --zookeeper s5:2181 --topic topic01 --from beginning  bin/kafka-console-consumer.sh --bootstrap-server s5:9092 --topic topic01 --from-beginning #offset数据不经过zk，而是通过kafka集群bootstrap-server #offset数据保存在kafka集群的topic里  查看topic信息 bin/kafka-topics.sh --zookeeper s5:2181 --describe topic topic01  Topic:topic01 PartitionCount:2 ReplicationFactor:2 Configs: Topic: topic01 Partition: 0 Leader: 1 Replicas: 1,0 Isr: 1 Topic: topic01 Partition: 1 Leader: 1 Replicas: 2,1 Isr: 1  删除topic bin/kafka-topics.sh --zookeeper s5:2181 --delete --topic topic01  ","date":"2020-11-26","permalink":"https://richard-chen93.github.io/post/kafka%E4%B8%8Ezookeeper%E7%9A%84%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/","tags":["technology"],"title":"Kafka与zookeeper的安装配置"},{"content":"下载、校验 wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.0.2.tar.gz sha1sum elasticsearch-5.0.2.tar.gz tar -xzf elasticsearch-5.0.2.tar.gz cd elasticsearch-5.0.2/ ./bin/elasticsearch\n验证 curl localhost:9200\n{ \u0026ldquo;name\u0026rdquo; : \u0026ldquo;Cp8oag6\u0026rdquo;, \u0026ldquo;cluster_name\u0026rdquo; : \u0026ldquo;elasticsearch\u0026rdquo;, \u0026ldquo;cluster_uuid\u0026rdquo; : \u0026ldquo;AT69_T_DTp-1qgIJlatQqA\u0026rdquo;, \u0026ldquo;version\u0026rdquo; : { \u0026ldquo;number\u0026rdquo; : \u0026ldquo;5.0.2\u0026rdquo;, \u0026ldquo;build_hash\u0026rdquo; : \u0026ldquo;f27399d\u0026rdquo;, \u0026ldquo;build_date\u0026rdquo; : \u0026ldquo;2016-03-30T09:51:41.449Z\u0026rdquo;, \u0026ldquo;build_snapshot\u0026rdquo; : false, \u0026ldquo;lucene_version\u0026rdquo; : \u0026ldquo;6.2.1\u0026rdquo; }, \u0026ldquo;tagline\u0026rdquo; : \u0026ldquo;You Know, for Search\u0026rdquo; }\nRunning as a daemon ./bin/elasticsearch -d -p pid\nshutdown ES kill cat pid\nkibana 相关 kinaba允许远程访问，配置文件中这里localhost为本机ip server.host: \u0026ldquo;10.3.3.30\u0026rdquo;\n启动kibana nohup ./kibana \u0026amp;\n","date":"2020-11-24","permalink":"https://richard-chen93.github.io/post/about_elasticsearch/","tags":["technology"],"title":"About_ElasticSearch"},{"content":"vim tab改为4个空格 cat \u0026gt;\u0026gt; /etc/vimrc \u0026lt;\u0026lt; EOF set ts=4 set expandtab set autoindent EOF  ","date":"2020-11-21","permalink":"https://richard-chen93.github.io/post/vim%E5%B8%B8%E8%A7%81%E7%94%A8%E6%B3%95%E5%92%8C%E9%85%8D%E7%BD%AE/","tags":["technology"],"title":"Vim常见用法和配置"},{"content":"环境准备  3台机器centos7，1台prometheus服务器，主机名S0，1台grafana服务器S1，1台客户端S2 prometheus版本为prometheus-2.5.0.linux-amd64 3台机器时间同步好  yum install ntpdate -y ntpdate cn.ntp.org.cn timedatectl set-timezone Asia/Shanghai date  在S0上安装Prometheus并运行 tar xf prometheus-2.5.0.linux-amd64.tar.gz mv prometheus-2.5.0.linux-amd64 /usr/local/prometheus cd /usr/local/prometheus ./prometheus --config.file=\u0026quot;/usr/local/prometheus/prometheus.yml\u0026quot; \u0026amp; ss -tunpl | grep 9090  若9090端口被Prometheus程序占用，说明启动成功，浏览器打开 http://10.3.3.30:9090/metrics可看到所有监控到的数据\n在被监控主机S2上安装node-exporter组件并运行 tar xf node_exporter-0.16.0.linux-amd64.tar.gz mv node_exporter-0.16.0.linux-amd64 /usr/local/node_exporter cd /usr/local/node_exporter/ nohup ./node_exporter \u0026amp; ss -tunpl | grep 9090  使用nohup后即使终端关闭，node_exporter程序也会继续运行 浏览器打开http://10.3.3.30:9100 http://10.3.3.32:9100/metrics可看到所有收集到的数据\n让Prometheus服务器拉取node节点信息 vim /home/admin/promethus/prometheus.yml - job_name: 'node01' static_configs: - targets: ['10.3.3.5:9100']  在文件末尾添加节点信息\n重启Prometheus服务并查看节点信息 pkill prometheus ./prometheus --config.file=\u0026quot;/home/admin/prometheus-2.5.0/prometheus.yml\u0026quot; \u0026amp;  添加mysql监控 在s2上安装启动mariadb，为监控系统添加账户mysql_monitor,密码123,授予权限。 安装使用mysqld_exporter组件,放到/usr/local/mysqld_exporter文件夹下面。\nmysql grant select,replication client,process ON *.* to 'mysql_monitor'@'localhost' identified by '123'; flush privileges; exit;  进入/usr/local/mysqld_exporter/, 修改mysql_exporter配置，添加mysql用户密码。然后启动mysql_exporter\nvim ./.my.cnf [client] user=mysql_monitor password=123 nohup ./mysqld_exporter --config.my-cnf=/home/admin/mysqld_exporter-0.12/my.cnf \u0026amp; lsof -i:9104  修改Prometheus服务器配置文件，添加mysql监控项。在主配置文件最后再添加下面三行：\n - job_name: 'nodeS2_mariadb' static_configs: - targets: ['10.3.3.6:9104']  安装grafana服务器软件 在s4上安装：\nwget https://dl.grafana.com/oss/release/grafana-5.3.4-1.x86_64.rpm sudo rpm -i --nodeps grafana-5.3.4-1.x86_64.rpm systemctl enable grafana-server systemctl start grafana-server ss -tunpl | grep 3000  浏览器打开http://10.3.3.31:3000 admin/admin 登录grafana后添加dashboard、编辑、选择数据源即可\n添加grafana图形监控模板： 下载图形监控模板 https://github.com/percona/grafana-dashboards 下载后压缩包里面dashboards文件夹里面的所有json文件就是图形监控模板文件。 在grafana中导入特定的json文件。\n设置数据库源 grafana监控界面里，configuration-data source，之前添加的数据源名称必须改为：Prometheus。然后mysql的监控就可以正常展示\n配置alertmanager Alertmanager 部署 普罗米修斯将数据采集和告警通知分成了两个模块。报警规则配置在普罗米修斯上（警报规则文件），然后发送报警信息到 AlertManger，AlertManager来管理这些报警信息，同时提供了聚合分组、告警抑制等高级功能，还支持通过 Email、WebHook 等多种方式发送告警消息提示。 （1）解压【alertmanager-0.21.0.linux-amd64.zip】压缩文件到指定目录 （2）进入目录，修改 alertmanager.yml 配置文件，该文件用于配置告警通知，这里提供的是163邮件的通知的样例 •\t你需要在这里配置上邮箱的 SMTP 服务器配置\n•\t在这里配置上告警通知的接收人。mail-error 表示严重等级的告警通知（比如服务宕机），mail-warning 表示紧急等级的告警通知（比如内存使用快满了）\n（3）默认端口为 9093，可通过修改sh脚本修改端口\n（4）如果修改了端口，需要在普罗米修斯的配置文件（prometheus.yml）中对应修改端口，然后重启普罗米修斯，使得普罗米修斯和 Alertmanager 可以正常通信。普罗米修斯prometheus.yml对应的alertmanager设置项为：\n# Alertmanager configuration alerting: alertmanagers: - static_configs: - targets: ['s4:9093'] rule_files: # 告警规则配置文件位置 - \u0026quot;rules/*.yml\u0026quot; - job_name: 'alertmanager' static_configs: - targets: ['s4:9093'] ./promtool check config prometheus.yml nohup ./alertmanager \u0026amp;  ","date":"2020-11-20","permalink":"https://richard-chen93.github.io/post/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2prometheus/","tags":["technology"],"title":"安装部署prometheus"},{"content":"docker以host模式启动容器，并指定名称和共享盘 docker container run -it \u0026ndash;name mhost \u0026ndash;privileged=true \u0026ndash;net=host -p 8000:80 -v /mhost_volume:/data/ {docker image id}\n进入容器 [root@localhost ~]# docker exec -it bash 进入后发现没有ifconfig，直接yum安装 ifconfig报错 yum install -y net-tools\n常用参数含义  it ：互动模式登录容器，并分配一个终端 name ：指定容器名称 p ：小p指定容器的80端口映射为宿主机的7879端口。 rm ：表示退出容器时，容器一起删除 v ：指定volumes，格式为： 宿主机共享目录：容器目录 ，这样宿主机的/ken目录就被挂载到了容器的/data/目录下了 \u0026ndash; privileged=true: 使共享的目录可以访问  将容器打包成镜像 docker commit \u0026ndash;change=\u0026lsquo;CMD [\u0026quot;/auto_sshd.sh\u0026quot;]\u0026rsquo; -c \u0026ldquo;EXPOSE 22\u0026rdquo; test-centos1 centos_sshd:7.0 命令注释： \u0026ndash;change : 将后期使用此镜像运行容器时的命令参数、开放的容器端口提前设置好。\n打包镜像到tar包 docker save -o centos7_django_mhost_v1.2.tar centos7_django_mhost_v1.2\n解压tar包到image docker load -i {image_name}.tar\ndocker查看、停止、删除容器 $ docker ps // 查看所有正在运行容器\n$ docker stop containerId // containerId 是容器的ID\n$ docker ps -a // 查看所有容器 $ docker ps -a -q // 查看所有容器ID\n$ docker stop $(docker ps -a -q) // stop停止所有容器\n$ docker rm $(docker ps -a -q) // remove删除所有容器\n","date":"2020-11-13","permalink":"https://richard-chen93.github.io/post/docker%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/","tags":["technology"],"title":"Docker常用操作"},{"content":"安装 harbor支持k8s的helm安装和本地安装，这里使用本地安装。\n1.\t前置条件 1.1安装docker并运行 yum install docker systemcal start docker systemctl enable docker\n2.\t安装docker-compost 2.1安装依赖包 2.1 yum install -y py-pip python-dev libffi-dev openssl-dev gcc libc-dev make\n2.2 curl\t-L \u0026ldquo;https://github.com/docker/compose/releases/download/1.23.2/docker-compose-$(uname -s)-$(uname -m)\u0026rdquo; -o /usr/local/bin/docker-compose\n2.3 chmod +x /usr/local/bin/docker-compose\n3.\t下载安装包 https://github.com/goharbor/harbor/releases tar xf harbor-offline-installer-v1.10.0.tgz\n4.\t修改配置文件 harbor.cfg 4.1修改hostname选项 hostname = A.B.C.D # 写你自己的网址或IP，公网访问要写公网IP 4.2支持Http 访问 customize_crt = false #新版中可能没有这项 注释掉https 4.3运行 4.3.1修改完配置文件后，运行 ./prepare，它会哪所配置文件修改一文件 如果是offline版的目录中有打包的image文件 docker load -i harbor.v1.10.0.tar.gz 4.3.2运行 ./install.sh 运行成功，docker container ls 查看，可以看到服务已经起来了。 docker container ls\nCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES da7981437516 goharbor/harbor-jobservice:v1.10.0 \u0026quot;/harbor/harbor_jobs…\u0026quot; 39 seconds ago Up 35 seconds (healthy) harbor-jobservice 534f615a8b49 goharbor/nginx-photon:v1.10.0 \u0026quot;nginx -g 'daemon of…\u0026quot; 39 seconds ago Up 34 seconds (healthy) 0.0.0.0:80-\u0026gt;8080/tcp nginx 6115684ab10b goharbor/harbor-core:v1.10.0 \u0026quot;/harbor/harbor_core\u0026quot; 40 seconds ago Up 38 seconds (healthy) harbor-core db6b18042976 goharbor/harbor-registryctl:v1.10.0 \u0026quot;/home/harbor/start.…\u0026quot; 43 seconds ago Up 40 seconds (healthy) registryctl 63c70e50cd7f goharbor/registry-photon:v2.7.1-patch-2819-2553-v1.10.0 \u0026quot;/home/harbor/entryp…\u0026quot; 43 seconds ago Up 39 seconds (healthy) 5000/tcp registry 46e4a59d052b goharbor/harbor-db:v1.10.0 \u0026quot;/docker-entrypoint.…\u0026quot; 43 seconds ago Up 40 seconds (healthy) 5432/tcp harbor-db 4ced2cd0ee8f goharbor/harbor-portal:v1.10.0 \u0026quot;nginx -g 'daemon of…\u0026quot; 43 seconds ago Up 39 seconds (healthy) 8080/tcp harbor-portal 691ab7bfb4bf goharbor/redis-photon:v1.10.0 \u0026quot;redis-server /etc/r…\u0026quot; 43 seconds ago Up 40 seconds (healthy) 6379/tcp redis 3431bbc1606e goharbor/harbor-log:v1.10.0 \u0026quot;/bin/sh -c /usr/loc…\u0026quot; 45 seconds ago Up 42 seconds (healthy) 127.0.0.1:1514-\u0026gt;10514/tcp harbor-log  常用管理命令 •\t停止服务： docker-compose stop •\t开始服务： docker-compose start Web登入 http://192.168.3.200 默认账号admin 密码Harbor12345 通过安装包中的harbor.yml 修改docker配置 docker 默认是按 https 请求的，由于我搭的私有库是 http 的，所以需要修改 docker 配置，将信任的库的地址写上 修改文件 /etc/docker/daemon.json { \u0026ldquo;insecure-registries\u0026rdquo;: [ \u0026ldquo;192.168.3.200\u0026rdquo; ] } systemctl restart docker 制作镜像 将 mongo 制作成一个私有镜像， mongo 为我之前从 docker hub 上拉取的镜像。 docker tag mongo A.B.C.D/ainirobot/nebulae_mongo:0.0.1\n上传镜像  先登陆私有库 docker login A.B.C.D PUSH docker push A.B.C.D/ainirobot/nebulae_mongo:0.0.1  ","date":"2020-11-13","permalink":"https://richard-chen93.github.io/post/harbor%E6%90%AD%E5%BB%BA%E4%B8%8E%E4%BD%BF%E7%94%A8/","tags":["technology"],"title":"Harbor搭建与使用"},{"content":"前提  windows被控端防火墙信任规则，允许5985端口通过。 对于旧版本windows 服务器需要 安装Framework 3.0+ 更改powershell策略为remotesigned 升级PowerShell至3.0+ 设置Windows远端管理，英文全称WS-Management（WinRM）  windows被控端设置： get-executionpolicy set-executionpolicy remotesigned winrm quickconfig -force winrm enumerate winrm/config/listener winrm e winrm/config/listener winrm set winrm/config/service/auth '@{Basic=\u0026quot;true\u0026quot;}' winrm set winrm/config/service '@{AllowUnencrypted=\u0026quot;true\u0026quot;}'  命令解释：  0.打开服务器运行脚本功能： 1 winrm service 默认都是未启用的状态，先查看状态；如无返回信息，则是没有启动；如果没启动，则打开 计算机管理\u0026gt;服务：启用 WRM服务 并设置为自动运行 2 针对winrm service 进行基础配置： 3 查看winrm service listener: 4 为winrm service 配置auth: 5 为winrm service 配置加密方式为允许非加密： 6 至此，winrm service 已经启用，可以正常使用；但是步骤4和5在windows重启后会失效，大概3分钟后会恢复，因为winrm服务默认设置为延迟启动。  linux控制端设置： yum -y install ansible yum -y install python-pip pip install --upgrade pip --trusted-host 10.193.194.101 pip install pywinrm --trusted-host 10.193.200.6 python -m pip install paramiko PyYAML Jinja2 httplib2 six  linux控制端添加被控主机清单，运行playbook剧本操作被控主机 vim /etc/ansible/hosts [Win_server] 192.168.1.105 ansible_ssh_user=\u0026quot;Administrator\u0026quot; ansible_ssh_pass=\u0026quot;123456\u0026quot; ansible_ssh_port=5985 ansible_connection=\u0026quot;winrm\u0026quot; ansible_winrm_server_cert_validation=ignore  ansible tserver -m win_ping vim /root/test.yml --- - hosts: tserver tasks: - name: push scripts win_copy : src=/root/config_disk.ps1 dest=C:\\\\config_disk.ps1 remote_user: Administrator - name: run scripts win_shell : C:\\config_disk.ps1 remote_user: Administrator  其他备注  vmwar虚拟机模板策略中，设置windows策略时，以下两条命令无效：  winrm set winrm/config/service/auth '@{Basic=\u0026quot;true\u0026quot;}' winrm set winrm/config/service '@{AllowUnencrypted=\u0026quot;true\u0026quot;}'  ","date":"2020-11-13","permalink":"https://richard-chen93.github.io/post/%E4%BD%BF%E7%94%A8ansible%E6%89%B9%E9%87%8F%E7%AE%A1%E7%90%86windows%E6%9C%8D%E5%8A%A1%E5%99%A8/","tags":["technology"],"title":"使用ansible批量管理windows服务器"},{"content":"目录  一、此实验中puppet的运行环境 1. 硬件清单 2. 软件清单 3. 准备yum源 4. 配置域名解析 5. Ntp时间同步 二、puppet安装配置过程 1. Puppet master安装配置 2. Puppet node安装配置 3. 分配证书，开启PUPPET服务 三、 申报站点清单中的资源，测试PUPPET能否工作。  正文 一、此实验中puppet的运行环境 1. 硬件清单  puppet master：4cpu，8G memory，1TB ssd puppet node1：4cpu，8G memory，500GB ssd  2. 软件清单 master\n 主机名: pmaster.cn04-corp.int OS: centos 7.6 puppet-master版本：3.6.2 ip：10.193.194.102  node\n 主机名: pnode1.cn04-corp.int OS: centos 7.6 puppet-master版本：3.6.2 ip：10.193.194.105  3. 准备yum源 Yum源服务器已配置好，可直接使用脚本配置到本地：\nwget http://10.193.200.6/yum.sh sh yum.sh  4. 配置域名解析 Puppet系统需要主机名或ip来标识master和node，目前两台机器均使用hosts文件做域名解析。\ncat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 10.193.194.102 pmaster.cn04-corp.int 10.193.194.105 pnode1.cn04-corp.int  5. Ntp时间同步 puppet master和node之间需要时间同步，Ntp服务器位于10.193.202.1。 puppet master和node都需要安装ntp服务并配置。\nyum install ntp vim /etc/ntp.conf  ntp配置文件里注释掉原有ntp服务器并添加10.193.202.1\n# Use public servers from the pool.ntp.org project. # Please consider joining the pool (http://www.pool.ntp.org/join.html). #server 0.centos.pool.ntp.org iburst #server 1.centos.pool.ntp.org iburst #server 2.centos.pool.ntp.org iburst #server 3.centos.pool.ntp.org iburst server 10.193.202.1 iburst vim /etc/ntp/step-tickers #0.centos.pool.ntp.org\t#注释掉 systemctl enable ntpd\t#开机自启ntp服务 systemctl start ntpd\t#启动ntp服务 ntpq -p\t#查看ntp服务器列表 date\t#查看时间是否一致  二、puppet安装配置过程 1. Puppet master安装配置 Yum install puppet-server\t#安装 Vim /etc/puppet/puppet.conf\t#修改配置文件 [agent]\t#这里添加server和certname server = pmaster.cn04-corp.int certname = pmaster.cn04-corp.int [master]\t#这里新建【master】，添加certname certname = pmaster.cn04-corp.int  2. Puppet node安装配置 Yum install puppet #安装pupet agent Vim /etc/puppet/puppet.conf\t#修改配置文件，添加以下master和certname [agent] server = pmaster.cn04-corp.int\t#指定master，这里是主机名 certname = pnode1.cn04-corp.int\t#指定node的证书名 runinterval = 60\t#设定agent请求catalog的间隔，这里是60秒。  3. 分配证书，开启PUPPET服务 在Node1上，输入以下命令向Master申请证书：\nPuppet agent -t  Master上查看待签发的证书,然后签发：\nPuppet cert list Puppet cert --sign pnode1.cn04-corp.int  三、 申报站点清单中的资源，测试PUPPET能否工作。 1. 开启puppet服务 Master：\nSystemctl enable puppetmaster Systemctl start puppetmaster  Node1：\nSystemctl enable puppet Systemctl start puppet  2. 申报站点清单中的资源，测试puppet能否工作。 Master上建立站点清单，位于/etc/puppet/manifest/site.pp 编辑site.pp，进行资源申报：\nvim /etc/puppet/manifest/site.pp node pnode1{\t#节点的主机名pnode1，不需要带域名.cn04-corp.int file {'test':\t#申报文件资源’test’，并定义各种状态 path=\u0026gt;'/root/test.txt', owner=\u0026gt;'root', group=\u0026gt;'root', mode=\u0026gt;'644', content=\u0026gt;'puppet system works!', } exec {'test shell scripts':\t#申报命令资源‘test shell scripts’，并定义各种状态 path=\u0026gt;'/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin', command=\u0026gt;'sh /root/sh/test.sh', user=\u0026gt;root, group=\u0026gt;root, } }  Node1 上设定了agent请求catalog的间隔为1分钟，若需立即执行，可用命令：\nPuppet agent -t  Node1执行完毕此命令后，查看Master的site.pp中定义的资源状态是否存在。\n[root@pnode1 ~]# ls /root/*.txt /root/name.txt /root/test.txt [root@pnode1 ~]#  存在说明puppet工作正常。\n附录 站点清单文件示例 文件名 site.pp 内容：\nnode pnode1{ file {'test': path=\u0026gt;'/root/test.txt', owner=\u0026gt;'root', group=\u0026gt;'root', mode=\u0026gt;'644', content=\u0026gt;'puppet system works!', } file {'test01': path=\u0026gt;'/root/test01.txt', owner=\u0026gt;'root', group=\u0026gt;'root', mode=\u0026gt;'644', content=\u0026gt;'puppet system works!', } exec {'test shell scripts': path=\u0026gt;'/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin', command=\u0026gt;'sh /root/sh/test.sh', user=\u0026gt;root, group=\u0026gt;root, } exec {'test command': path=\u0026gt;'/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin', command=\u0026gt;'touch /root/exec.txt', user=\u0026gt;root, group=\u0026gt;root, } }  ","date":"2020-11-13","permalink":"https://richard-chen93.github.io/post/%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4%E5%B7%A5%E5%85%B7puppet%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/","tags":["technology"],"title":"自动化运维工具puppet简单使用说明"},{"content":"linux更改 ssh 私钥 Passphrase $ cd ~/.ssh $ ssh-keygen -f id_rsa -p  ssh连接速度慢 修改sshd_config文件： UseDNS no GSSAPIAuthentication no\n实现免密登陆 1、在客户端生成密钥对,默认在~/.ssh文件夹下 ssh-keygen 2、将公钥上传到服务器 ssh-copy-id root@10.0.0.1 3、验证。\n","date":"2020-10-29","permalink":"https://richard-chen93.github.io/post/ssh/","tags":["technology"],"title":"ssh"},{"content":"docker-compose\n1.安装扩展源\nsudo yum -y install epel-release\n2.安装python-pip模块\nsudo yum install python-pip\n3.查看docker-compose版本\ndocker-compose version\n提示未找到命令 4.通过命令进行安装\ncd /usr/local/bin/\nwget https://github.com/docker/compose/releases/download/1.14.0-rc2/docker-compose-Linux-x86_64\nrename docker-compose-Linux-x86_64 docker-compose docker-compose-Linux-x86_64\nchmod +x /usr/local/bin/docker-compose\n5.再通过docker-compose version命令进行查看\n","date":"2020-10-29","permalink":"https://richard-chen93.github.io/post/docker-compose%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/","tags":["technology"],"title":"Docker Compose"},{"content":"Git中submodule的使用\n面对比较复杂的项目，我们有可能会将代码根据功能拆解成不同的子模块。主项目对子模块有依赖关系，却又并不关心子模块的内部开发流程细节。\n这种情况下，通常不会把所有源码都放在同一个 Git 仓库中。\n有一种比较简单的方式，是在当前工作目录下，将子模块文件夹加入到 .gitignore 文件内容中，这样主项目就能够无视子项目的存在。这样做有一个弊端就是，使用主项目的人需要有一个先验知识：需要在当前目录下放置一份某版本的子模块代码。\n还有另外一种方式可供借鉴，可以使用 Git 的 submodule 功能，也是这篇文章的主题。\n实际上 Git 工具的 submodule 功能就是建立了当前项目与子模块之间的依赖关系：子模块路径、子模块的远程仓库、子模块的版本号。\n使用流程 假定我们有两个项目：project-main 和 project-sub-1，其中 project-main 表示主项目，而 project-sub-1 表示子模块项目。\n其中 project-main 的远程仓库地址为 https://github.com/username/project-main.git，而 project-sub-1 的远程仓库地址为 https://github.com/username/project-sub-1.git。\n接下来，我们希望在 project-main 中添加 project-sub-1 ，而又保持 project-sub-1 自身独立的版本控制。\n1.创建 submodule 使用 git submodule add \u0026lt;submodule_url\u0026gt; 命令可以在项目中创建一个子模块。\n进入项目 project-main ，输入：\n➜ project-main git:(master) git submodule add https://github.com/username/project-sub-1.git 正克隆到 \u0026lsquo;/path/to/project-main/project-sub-1\u0026rsquo;\u0026hellip; remote: Enumerating objects: 3, done. remote: Counting objects: 100% (3/3), done. remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 展开对象中: 100% (3/3), 完成. 此时项目仓库中会多出两个文件：.gitmodules 和 project-sub-1 。\n前者的内容是这样的，事实上就是子模块的相关信息；而后者那个文件，实际上保存的是子模块当前版本的版本号信息。\n[submodule \u0026ldquo;project-sub-1\u0026rdquo;] path = project-sub-1 url = https://github.com/username/project-sub-1.git 如果此前项目中已经存在 .gitmodules 文件，则会在文件内容中多出上述三行记录。\n事实上，此时在 .git/config 文件中也会多出一些信息，在 .git/modules 文件夹下也会多出一份内容。\n通常此时可以使用 git commit -m \u0026ldquo;add submodule xxx\u0026rdquo; 提交一次，表示引入了某个子模块。提交后，在主项目仓库中，会显示出子模块文件夹，并带上其所在仓库的版本号。\n2.获取 submodule 上述步骤在创建子模块的过程中，会自动将相关代码克隆到对应路径，但对于后续使用者而言，对于主项目使用普通的 clone 操作并不会拉取到子模块中的实际代码。\n使用以下命令进行克隆，完成后 project-main/project-sub-1 文件夹是空的：\ncd /path/to/temp git clone https://github.com/username/project-main.git\n如果希望子模块代码也获取到，一种方式是在克隆主项目的时候带上参数 \u0026ndash;recurse-submodules，这样会递归地将项目中所有子模块的代码拉取。\ncd /path/to/temp2 git clone https://github.com/username/project-main.git \u0026ndash;recurse-submodules 此时 project-main/project-sub-1 文件夹是有内容的，并且固定在某个 Git 提交的版本上。\n另外一种可行的方式是，在当前主项目中执行：\ngit submodule init git submodule update 则会根据主项目的配置信息，拉取更新子模块中的代码。\n3.子模块内容的更新 对于子模块而言，并不需要知道引用自己的主项目的存在。对于自身来讲，子模块就是一个完整的 Git 仓库，按照正常的 Git 代码管理规范操作即可。\n对于主项目而言，子模块的内容发生变动时，通常有三种情况：\n1）当前项目下子模块文件夹内的内容发生了未跟踪的内容变动；\n2）当前项目下子模块文件夹内的内容发生了版本变化；\n3）当前项目下子模块文件夹内的内容没变，远程有更新；\n 情况1：子模块有未跟踪的内容变动 对于第1种情况，通常是在开发环境中，直接修改子模块文件夹中的代码导致的。\n 此时在主项目中使用 git status 能够看到关于子模块尚未暂存以备提交的变更，但是于主项目而言是无能为力的，使用 git add/commit 对其也不会产生影响。\n➜ project-main git:(master) git status 位于分支 master 您的分支与上游分支 \u0026lsquo;origin/master\u0026rsquo; 一致。 尚未暂存以备提交的变更： （使用 \u0026ldquo;git add \u0026lt;文件\u0026gt;\u0026hellip;\u0026rdquo; 更新要提交的内容） （使用 \u0026ldquo;git checkout \u0026ndash; \u0026lt;文件\u0026gt;\u0026hellip;\u0026rdquo; 丢弃工作区的改动） （提交或丢弃子模组中未跟踪或修改的内容） 修改： project-sub-1 (未跟踪的内容) 修改尚未加入提交（使用 \u0026ldquo;git add\u0026rdquo; 和/或 \u0026ldquo;git commit -a\u0026rdquo;） 在此情景下，通常需要进入子模块文件夹，按照子模块内部的版本控制体系提交代码。\n当提交完成后，主项目的状态则进入了情况2，即当前项目下子模块文件夹内的内容发生了版本变化。\n 情况2：子模块有版本变化 当子模块版本变化时，在主项目中使用 git status 查看仓库状态时，会显示子模块有新的提交：\n ➜ project-main git:(master) ✗ git status 位于分支 master 您的分支与上游分支 \u0026lsquo;origin/master\u0026rsquo; 一致。 尚未暂存以备提交的变更： （使用 \u0026ldquo;git add \u0026lt;文件\u0026gt;\u0026hellip;\u0026rdquo; 更新要提交的内容） （使用 \u0026ldquo;git checkout \u0026ndash; \u0026lt;文件\u0026gt;\u0026hellip;\u0026rdquo; 丢弃工作区的改动） 修改： project-sub-1 (新提交) 修改尚未加入提交（使用 \u0026ldquo;git add\u0026rdquo; 和/或 \u0026ldquo;git commit -a\u0026rdquo;） 在这种情况下，可以使用 git add/commit 将其添加到主项目的代码提交中，实际的改动就是那个子模块 文件 所表示的版本信息：\ngit diff HEAD HEAD^ diff \u0026ndash;git a/project-sub-1 b/project-sub-1 index ace9770..7097c48 160000 \u0026mdash; a/project-sub-1 +++ b/project-sub-1 @@ -1 +1 @@ -Subproject commit ace977071f94f4f88935f9bb9a33ac0f8b4ba935 +Subproject commit 7097c4887798b71cee360e99815f7dbd1aa17eb4 通常当子项目更新后，主项目修改其所依赖的版本时，会产生类似这种情景的 commit 提交信息。\n 情况3：子模块远程有更新 通常来讲，主项目与子模块的开发不会恰好是同时进行的。通常是子模块负责维护自己的版本升级后，推送到远程仓库，并告知主项目可以更新对子模块的版本依赖。\n 在这种情况下，主项目是比较茫然的。\n之前曾经提到，主项目可以使用 git submodule update 更新子模块的代码，但那是指 当前主项目文件夹下的子模块目录内容 与 当前主项目记录的子模块版本 不一致时，会参考后者进行更新。\n但如今这种情况下，后者 当前主项目记录的子模块版本 还没有变化，在主项目看来当前情况一切正常。\n此时，需要让主项目主动进入子模块拉取新版代码，进行升级操作。\n通常流程是：\ncd project-sub-1 git pull origin master 子模块目录下的代码版本会发生变化，转到情况2的流程进行主项目的提交。\n当主项目的子项目特别多时，可能会不太方便，此时可以使用 git submodule 的一个命令 foreach 执行：\ngit submodule foreach \u0026lsquo;git pull origin master\u0026rsquo;\n 情况汇总 终上所述，可知在不同场景下子模块的更新方式如下：\n 对于子模块，只需要管理好自己的版本，并推送到远程分支即可； 对于父模块，若子模块版本信息未提交，需要更新子模块目录下的代码，并执行 commit 操作提交子模块版本信息； 对于父模块，若子模块版本信息已提交，需要使用 git submodule update ，Git 会自动根据子模块版本信息更新所有子模块目录的相关代码。\n4.删除子模块 网上流传了一些偏法，主要步骤是直接移除模块，并手动修改 .gitmodules、.git/config 和 .git/modules 内容。包含了一大堆类似git rm \u0026ndash;cached 、rm -rf 、rm .gitmodules 和 git rm \u0026ndash;cached 之类的代码。\n实际上这是一种比较野的做法，不建议使用。\n根据官方文档的说明，应该使用 git submodule deinit 命令卸载一个子模块。这个命令如果添加上参数 \u0026ndash;force，则子模块工作区内即使有本地的修改，也会被移除。\ngit submodule deinit project-sub-1 git rm project-sub-1 执行 git submodule deinit project-sub-1 命令的实际效果，是自动在 .git/config 中删除了以下内容：\n[submodule \u0026ldquo;project-sub-1\u0026rdquo;] url = https://github.com/username/project-sub-1.git 执行 git rm project-sub-1 的效果，是移除了 project-sub-1 文件夹，并自动在 .gitmodules 中删除了以下内容：\n[submodule \u0026ldquo;project-sub-1\u0026rdquo;] path = project-sub-1 url = https://github.com/username/project-sub-1.git 此时，主项目中关于子模块的信息基本已经删除（虽然貌似 .git/modules 目录下还有残余）：\n➜ project-main git:(master) ✗ gs 位于分支 master 您的分支与上游分支 \u0026lsquo;origin/master\u0026rsquo; 一致。 要提交的变更： （使用 \u0026ldquo;git reset HEAD \u0026lt;文件\u0026gt;\u0026hellip;\u0026rdquo; 以取消暂存） 修改： .gitmodules 删除： project-sub-1 可以提交代码：\ngit commit -m \u0026ldquo;delete submodule project-sub-1\u0026rdquo; 至此完成对子模块的删除。\n总结 当项目比较复杂，部分代码希望独立为子模块进行版本控制时，可以使用 git submodule 功能。\n使用 git submodule 功能时，主项目仓库并不会包含子模块的文件，只会保留一份子模块的配置信息及版本信息，作为主项目版本管理的一部分。\n本篇文章简单介绍了 git submodule 的添加和删除，以及项目开发过程中主项目与子模块不同状态时刻的操作方式。\n","date":"2020-10-29","permalink":"https://richard-chen93.github.io/post/submodule%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/","tags":["technology"],"title":"git Submodule"},{"content":"hugo本地创建、更新、删除文章后同步到gitpage的基本流程： 前提环境： public目录位于blog目录下，属于blog的子仓库submodule 使用命令 git submodule status可看到 注意 写文章后直接sh deploy.sh，不用单独运行hugo相关任何命令\n结论，改动blog仓库之前，确保public子仓库所有改动已提交 已push 问题： 如果仓库有子模块，任何情况下都先确保子模块commit和push以后，才可以同步父仓库？否则子模块就失效？\n1、写文章 在blog项目根目录下执行 hugo new post/test.md 创建了一个md文件 vim content/post/test.md 移除 draft: true这一行 否则草稿不会公开为文章\n2、发布文章到gitpage blog根目录下执行deploy.sh脚本，成功以后等待约1分钟，gitpage上即可看到更新后的内容\n3、修改、删除文章 只需要编辑或删除blog/content/post/下的md文件，然后再次在根目录执行deploy.sh脚本，即可同步到gitpage\n4、同步本地blog仓库文件到github 任何本地blog根目录的文件，包括content/post下的md文件，或者config.toml配置文件，更新后都可以执行second-push.sh，远端仓库立即生效。second-push.sh内容为（git add \u0026ndash;all . \u0026amp;\u0026amp; git commit -m \u0026ldquo;update\u0026rdquo; \u0026amp;\u0026amp; git push）\n5、其他 对md文章或config.toml做任何改动以后，首先需要执行deploy.sh。然后如有需求再执行second-push.sh\n其他：克隆此仓库 git clone git@github.com:richard-chen93/blog.git\n其他 待删除，可以尝试的命令： git submodule sync git submodule init git submodule update\n添加tags 博客根目录下的archetypes目录下，也有一个default.md文件。这是hugo新建md文件的默认模板\n启动实时预览（本地预览网站效果） 写一篇文章生成一次会很繁琐，可以通过启动网站预览，实时监控页面的更改并刷新页面。 hugo server -D 参数： -D 输出包括标记为 draft: true 的草稿文章\n默认地址为 http://localhost:1313 如果 1313 端口被占用，会随机使用其他空端口。\n若换了新电脑，要在新电脑上发布文章  1、将blog克隆到本地  git clone git@github.com:richard-chen93/blog.git   2、进入blog根目录，删除public文件夹  rm -rf public   3、用以下命令设置子模块   git submodule init git submodule update git submodule status git submodule sync  此时执行deploy可能会报错： fatal: You are not currently on a branch. To push the history leading to the current (detached HEAD) state now, use\ngit push origin HEAD:\u0026lt;name-of-remote-branch\u0026gt;  可尝试下面的指令修复问题：(在blog目录或public目录下都做)\n git checkout main git push origin HEAD:main git push -f  如果再有如下报错：\nAuto-merging search/index.json CONFLICT (content): Merge conflict in search/index.json Auto-merging post/index.html Auto-merging index.html Auto-merging archives/index.html Automatic merge failed; fix conflicts and then commit the result.  这样处理：\ngit add search/index.json git commit -s git push  Hugo 添加网站流量统计 不蒜子是一个通过仅仅两行代码实现的网页流量计数器\n1.在themes/layouts/partials/head.html文件中引入不蒜子js文件\n\u0026lt;!-- 不蒜子 --\u0026gt; \u0026lt;script async src=\u0026quot;//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt;  2.在页面添加统计代码，在/themes/layouts/partials/footer.html中添加如下代码\n\u0026lt;span id=\u0026quot;busuanzi_container_site_pv\u0026quot;\u0026gt; 本站总访问量\u0026lt;span id=\u0026quot;busuanzi_value_site_pv\u0026quot;\u0026gt;\u0026lt;/span\u0026gt;次 \u0026lt;/span\u0026gt; \u0026amp;nbsp;  3.在themes/layouts/_default/single.html中添加以下代码\n\u0026lt;h5 id=\u0026quot;wc\u0026quot; style=\u0026quot;font-size: 1rem;text-align: center;\u0026quot;\u0026gt;{{ .FuzzyWordCount }} Words|Read in about {{ .ReadingTime }} Min|\u0026lt;span id=\u0026quot;busuanzi_container_page_pv\u0026quot;\u0026gt; 本文总阅读量\u0026lt;span id=\u0026quot;busuanzi_value_page_pv\u0026quot;\u0026gt;\u0026lt;/span\u0026gt;次 \u0026lt;/span\u0026gt;\u0026lt;/h5\u0026gt;  可根据个人喜好选择放在文章头部或尾部\n问题记录 执行 hugo \u0026ndash;cleanDestinationDir, 若blog仓库content/post下有删除的md文章，则public/post下对应的html文章也会同步删除。然后执行deploy.sh之后，git就会报错：\n # On branch main # Untracked files: # (use \u0026quot;git add \u0026lt;file\u0026gt;...\u0026quot; to # include in what will be # committed) # ../content/post/1.md nothing added to commit but untracked files present (use \u0026quot;git add\u0026quot; to track)  所以目前不要动public目录下的任何东西，更新文章只在blog下进行，再deploy到gitpage即可。\n","date":"2020-10-28","permalink":"https://richard-chen93.github.io/post/hugo%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95/","tags":["technology"],"title":"Hugo基本用法"},{"content":"将hugo静态博客网页部署到github上 1、选择gitpage的类型 2种github page：个人、项目。这里选个人。\n2、建立gitpage和hugo代码仓库 在github上以自己的用户名+github.io为名建立page的仓库，如我的用户名richard-chen93, 仓库名为richard-chen93.github.io 创建一个仓库用于存放hugo的代码，例如取名为blog。将此blog克隆至本地。将本地能正常运行的hugo站点文件拷贝到blog。 注意：新建立的github.io仓库（任何新建的仓库都建议先添加一个文件，比如readme.md）务必先提交一次再使用submodule。如现在github 页面上进入此仓库，随便新建任何一个文件，点提交。否则仓库可能无法使用或无法添加子仓库。\n3、删除public目录，建立页面文件的子仓库。 删除blog项目根目录下的public目录,并建立git子仓库\nrm -rf public git submodule add -b main git@github.com:richard-chen93/richard-chen93.github.io.git public  建立子仓库后，当你执行hugo命令，生成public下的页面文件时，public目录会有一个不同的远程源\n4、修改hugo配置文件 在hugo站点配置文件 config.toml中，baseurl设置为你的站点名称，如 https://richard-chen93.github.io\n5、部署脚本，直接拿来用 在blog根目录执行hugo.exe命令后，会在public文件夹生成页面文件。使用deploy.sh脚本即可推送到github上\n#!/bin/sh # If a command fails then the deploy stops set -e printf \u0026quot;\\033[0;32mDeploying updates to GitHub...\\033[0m\\n\u0026quot; # Build the project. hugo ## if using a theme, replace with `hugo -t \u0026lt;YOURTHEME\u0026gt;` # Go To Public folder cd public # Add changes to git. git add . # Commit changes. msg=\u0026quot;rebuilding site $(date)\u0026quot; if [ -n \u0026quot;$*\u0026quot; ]; then msg=\u0026quot;$*\u0026quot; fi git commit -m \u0026quot;$msg\u0026quot; # Push source and build repos. git push origin master  可以使用deploy.sh ＋ commit message来提交\n至此几分钟后即可打开你的gitpage页面。 https://richard-chen93.github.io\n","date":"2020-10-27","permalink":"https://richard-chen93.github.io/post/deploy-hugo-site_on-github-pages/","tags":["technology"],"title":"Deploy Hugo Site_on Github Pages"},{"content":"1 无法push A git directory for \u0026lsquo;public\u0026rsquo; is found locally with remote(s): origin https://github.com/richard-chen93/.........git If you want to reuse this local git directory instead of cloning again from https://github.com/richard-chen93/......git use the \u0026lsquo;\u0026ndash;force\u0026rsquo; option. If the local git directory is not the correct repo or you are unsure what this means choose another name with the \u0026lsquo;\u0026ndash;name\u0026rsquo; option.\n删除.git\\modules 的 public\n2 git push时总让输入密码 git push的时候每次都要输入用户名和密码的问题解决 换了个ssh key,发现每次git push origin master的时候都要输入用户名和密码 原因是在添加远程库的时候使用了https的方式。。所以每次都要用https的方式push到远程库 查看使用的传输协议: git remote -v\ngit remote rm origin git remote add origin git@github.com:username/repository.git git push -u origin master\ngit remote -v\n无法push git push -u origin master error: src refspec master does not match any error: failed to push some refs to \u0026lsquo;github.com:richard-chen93/hugo.git\u0026rsquo;\nreset \u0026ndash;hard：重置stage区和工作目录: reset \u0026ndash;hard 会在重置 HEAD 和branch的同时，重置stage区和工作目录里的内容。当你在 reset 后面加了 \u0026ndash;hard 参数时，你的stage区和工作目录里的内容会被完全重置为和HEAD的新位置相同的内容。换句话说，就是你的没有commit的修改会被全部擦掉。\n例如你在上次 commit 之后又对文件做了一些改动：把修改后的ganmes.txt文件add到stage区，修改后的shopping list.txt保留在工作目录\n05 git submodule add error: does not have a commit checked out 1、新建的仓库，要至少提交一次更改（比如直接在github web页面随便添加一个任何文件，然后点提交。） 2、删除public文件夹。\n06 fatal: You are not currently on a branch. To push the history leading to the current (detached HEAD) state now, use git push origin HEAD: 运行命令(在blog目录 和public目录下都执行此操作) git checkout main 即可解决\n07 如下报错：\nAuto-merging search/index.json CONFLICT (content): Merge conflict in search/index.json Auto-merging post/index.html Auto-merging index.html Auto-merging archives/index.html Automatic merge failed; fix conflicts and then commit the result.  处理方法：\ngit add search/index.json git commit -s git push  ​\n","date":"2020-10-27","permalink":"https://richard-chen93.github.io/post/git%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%E6%B1%87%E6%80%BB/","tags":["technology"],"title":"Git常见错误汇总"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution  Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\n Blockquote with attribution  Don\u0026rsquo;t communicate by sharing memory, share memory by communicating.\n— Rob Pike1\n Tables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\n   Name Age     Bob 27   Alice 23    Inline Markdown within tables    Inline  Markdown  In  Table     italics bold strikethrough  code    Code Blocks Code block with backticks html\r\u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026quot;UTF-8\u0026quot; /\u0026gt;\r\u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r Code block indented with four spaces \u0026lt;!DOCTYPE html\u0026gt;\r\u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt;\r\u0026lt;head\u0026gt;\r\u0026lt;meta charset=\u0026quot;UTF-8\u0026quot;\u0026gt;\r\u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt;\r\u0026lt;/head\u0026gt;\r\u0026lt;body\u0026gt;\r\u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt;\r\u0026lt;/body\u0026gt;\r\u0026lt;/html\u0026gt;\r List Types Ordered List  First item Second item Third item  Unordered List  List item Another item And another item  Nested list  Item   First Sub-item Second Sub-item  Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\n  The above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015. \u0026#x21a9;\u0026#xfe0e;\n  ","date":"2020-05-11","permalink":"https://richard-chen93.github.io/post/markdown-syntax/","tags":["test"],"title":"Markdown Syntax"},{"content":"Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\nIn this example we will be using KaTeX\n Create a partial under /layouts/partials/math.html Within this partial reference the Auto-render Extension or host these scripts locally. Include the partial in your templates like so:  {{ if or .Params.math .Site.Params.math }} {{ partial \u0026quot;math.html\u0026quot; . }} {{ end }}   To enable KaTex globally set the parameter math to true in a project\u0026rsquo;s configuration To enable KaTex on a per page basis include the parameter math: true in content files.  Note: Use the online reference of Supported TeX Functions Examples Inline math: $$ \\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887… $$\nBlock math:\n$$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$\n","date":"2020-03-08","permalink":"https://richard-chen93.github.io/post/math-typesetting/","tags":["test"],"title":"Math Typesetting"},{"content":"Lorem est tota propiore conpellat pectoribus de pectora summo. Redit teque digerit hominumque toris verebor lumina non cervice subde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc caluere tempus inhospita parcite confusaque translucet patri vestro qui optatis lumine cognoscere flos nubis! Fronde ipsamque patulos Dryopen deorum.\n Exierant elisi ambit vivere dedere Duce pollice Eris modo Spargitque ferrea quos palude  Rursus nulli murmur; hastile inridet ut ab gravi sententia! Nomine potitus silentia flumen, sustinet placuit petis in dilapsa erat sunt. Atria tractus malis.\n Comas hunc haec pietate fetum procerum dixit Post torum vates letum Tiresia Flumen querellas Arcanaque montibus omnes Quidem et  Vagus elidunt \nThe Van de Graaf Canon\nMane refeci capiebant unda mulcebat Victa caducifer, malo vulnere contra dicere aurato, ludit regale, voca! Retorsit colit est profanae esse virescere furit nec; iaculi matertera et visa est, viribus. Divesque creatis, tecta novat collumque vulnus est, parvas. Faces illo pepulere tempus adest. Tendit flamma, ab opes virum sustinet, sidus sequendo urbis.\nIubar proles corpore raptos vero auctor imperium; sed et huic: manus caeli Lelegas tu lux. Verbis obstitit intus oblectamina fixis linguisque ausus sperare Echionides cornuaque tenent clausit possit. Omnia putatur. Praeteritae refert ausus; ferebant e primus lora nutat, vici quae mea ipse. Et iter nil spectatae vulnus haerentia iuste et exercebat, sui et.\nEurytus Hector, materna ipsumque ut Politen, nec, nate, ignari, vernum cohaesit sequitur. Vel mitis temploque vocatus, inque alis, oculos nomen non silvis corpore coniunx ne displicet illa. Crescunt non unus, vidit visa quantum inmiti flumina mortis facto sic: undique a alios vincula sunt iactata abdita! Suspenderat ego fuit tendit: luna, ante urbem Propoetides parte.\n","date":"2019-03-09","permalink":"https://richard-chen93.github.io/post/placeholder-text/","tags":["test"],"title":"Placeholder Text"},{"content":"The following is part of the CJK text, this page is for test use only.\nCJK Radicals Supplement ⺀ ⺁ ⺂ ⺃ ⺄ ⺅ ⺆ ⺇ ⺈ ⺉ ⺊ ⺋ ⺌ ⺍ ⺎ ⺏ ⺐ ⺑ ⺒ ⺓ ⺔ ⺕ ⺖ ⺗ ⺘ ⺙ ⺛ ⺜ ⺝ ⺞ ⺟ ⺠ ⺡ ⺢ ⺣ ⺤ ⺥ ⺦ ⺧ ⺨ ⺩ ⺪ ⺫ ⺬ ⺭ ⺮ ⺯ ⺰ ⺱ ⺲ ⺳ ⺴ ⺵ ⺶ ⺷ ⺸ ⺹ ⺺ ⺻ ⺼ ⺽ ⺾ ⺿ ⻀ ⻁ ⻂ ⻃ ⻄ ⻅ ⻆ ⻇ ⻈ ⻉ ⻊ ⻋ ⻌ ⻍ ⻎ ⻏ ⻐ ⻑ ⻒ ⻓ ⻔ ⻕ ⻖ ⻗ ⻘ ⻙ ⻚ ⻛ ⻜ ⻝ ⻞ ⻟ ⻠ ⻡ ⻢ ⻣ ⻤ ⻥ ⻦ ⻧ ⻨ ⻩ ⻪ ⻫ ⻬ ⻭ ⻮ ⻯ ⻰ ⻱ ⻲ ⻳\nKangxi Radicals ⼀ ⼁ ⼂ ⼃ ⼄ ⼅ ⼆ ⼇ ⼈ ⼉ ⼊ ⼋ ⼌ ⼍ ⼎ ⼏ ⼐ ⼑ ⼒ ⼓ ⼔ ⼕ ⼖ ⼗ ⼘ ⼙ ⼚ ⼛ ⼜ ⼝ ⼞ ⼟ ⼠ ⼡ ⼢ ⼣ ⼤ ⼥ ⼦ ⼧ ⼨ ⼩ ⼪ ⼫ ⼬ ⼭ ⼮ ⼯ ⼰ ⼱ ⼲ ⼳ ⼴ ⼵ ⼶ ⼷ ⼸ ⼹ ⼺ ⼻ ⼼ ⼽ ⼾ ⼿ ⽀ ⽁ ⽂ ⽃ ⽄ ⽅ ⽆ ⽇ ⽈ ⽉ ⽊ ⽋ ⽌ ⽍ ⽎ ⽏ ⽐ ⽑ ⽒ ⽓ ⽔ ⽕ ⽖ ⽗ ⽘ ⽙ ⽚ ⽛ ⽜ ⽝ ⽞ ⽟ ⽠ ⽡ ⽢ ⽣ ⽤ ⽥ ⽦ ⽧ ⽨ ⽩ ⽪ ⽫ ⽬ ⽭ ⽮ ⽯ ⽰ ⽱ ⽲ ⽳ ⽴ ⽵ ⽶ ⽷ ⽸ ⽹ ⽺ ⽻ ⽼ ⽽ ⽾ ⽿ \u0026hellip;\nCJK Symbols and Punctuation 、 。 〃 〄 々 〆 〇 〈 〉 《 》 「 」 『 』 【 】 〒 〓 〔 〕 〖 〗 〘 〙 〚 〛 〜 〝 〞 〟 〠 〡 〢 〣 〤 〥 〦 〧 〨 〩 〪 〫 〬 〭 〮 〯 〰 〱 〲 〳 〴 〵 〶 〷 〸 〹 〺 〻 〼 〽 〾 〿\nHiragana ぁ あ ぃ い ぅ う ぇ え ぉ お か が き ぎ く ぐ け げ こ ご さ ざ し じ す ず せ ぜ そ ぞ た だ ち ぢ っ つ づ て で と ど な に ぬ ね の は ば ぱ ひ び ぴ ふ ぶ ぷ へ べ ぺ ほ ぼ ぽ ま み む め も ゃ や ゅ ゆ ょ よ ら り る れ ろ ゎ わ ゐ ゑ を ん ゔ ゕ ゖ ゙ ゚ ゛ ゜ ゝ ゞ ゟ\nKatakana ゠ ァ ア ィ イ ゥ ウ ェ エ ォ オ カ ガ キ ギ ク グ ケ ゲ コ ゴ サ ザ シ ジ ス ズ セ ゼ ソ ゾ タ ダ チ ヂ ッ ツ ヅ テ デ ト ド ナ ニ ヌ ネ ノ ハ バ パ ヒ ビ ピ フ ブ プ ヘ ベ ペ ホ ボ ポ マ ミ ム メ モ ャ ヤ ュ ユ ョ ヨ ラ リ ル レ ロ ヮ ワ ヰ ヱ ヲ ン ヴ ヵ ヶ ヷ ヸ ヹ ヺ ・ ー ヽ ヾ ヿ\nBopomofo ㄅ ㄆ ㄇ ㄈ ㄉ ㄊ ㄋ ㄌ ㄍ ㄎ ㄏ ㄐ ㄑ ㄒ ㄓ ㄔ ㄕ ㄖ ㄗ ㄘ ㄙ ㄚ ㄛ ㄜ ㄝ ㄞ ㄟ ㄠ ㄡ ㄢ ㄣ ㄤ ㄥ ㄦ ㄧ ㄨ ㄩ ㄪ ㄫ ㄬ\nHangul Compatibility Jamo ㄱ ㄲ ㄳ ㄴ ㄵ ㄶ ㄷ ㄸ ㄹ ㄺ ㄻ ㄼ ㄽ ㄾ ㄿ ㅀ ㅁ ㅂ ㅃ ㅄ ㅅ ㅆ ㅇ ㅈ ㅉ ㅊ ㅋ ㅌ ㅍ ㅎ ㅏ ㅐ ㅑ ㅒ ㅓ ㅔ ㅕ ㅖ ㅗ ㅘ ㅙ ㅚ ㅛ ㅜ ㅝ ㅞ ㅟ ㅠ ㅡ ㅢ ㅣ ㅤ ㅥ ㅦ ㅧ ㅨ ㅩ ㅪ ㅫ ㅬ ㅭ ㅮ ㅯ ㅰ ㅱ ㅲ ㅳ ㅴ ㅵ ㅶ ㅷ ㅸ ㅹ ㅺ ㅻ ㅼ ㅽ ㅾ ㅿ ㆀ ㆁ ㆂ ㆃ ㆄ ㆅ ㆆ ㆇ ㆈ ㆉ ㆊ ㆋ ㆌ ㆍ ㆎ\nKanbun ㆐ ㆑ ㆒ ㆓ ㆔ ㆕ ㆖ ㆗ ㆘ ㆙ ㆚ ㆛ ㆜ ㆝ ㆞ ㆟\nBopomofo Extended ㆠ ㆡ ㆢ ㆣ ㆤ ㆥ ㆦ ㆧ ㆨ ㆩ ㆪ ㆫ ㆬ ㆭ ㆮ ㆯ ㆰ ㆱ ㆲ ㆳ ㆴ ㆵ ㆶ ㆷ\nKatakana Phonetic Extensions ㇰ ㇱ ㇲ ㇳ ㇴ ㇵ ㇶ ㇷ ㇸ ㇹ ㇺ ㇻ ㇼ ㇽ ㇾ ㇿ\nEnclosed CJK Letters and Months ㈀ ㈁ ㈂ ㈃ ㈄ ㈅ ㈆ ㈇ ㈈ ㈉ ㈊ ㈋ ㈌ ㈍ ㈎ ㈏ ㈐ ㈑ ㈒ ㈓ ㈔ ㈕ ㈖ ㈗ ㈘ ㈙ ㈚ ㈛ ㈜ ㈠ ㈡ ㈢ ㈣ ㈤ ㈥ ㈦ ㈧ ㈨ ㈩ ㈪ ㈫ ㈬ ㈭ ㈮ ㈯ ㈰ ㈱ ㈲ ㈳ ㈴ ㈵ ㈶ ㈷ ㈸ ㈹ ㈺ ㈻ ㈼ ㈽ ㈾ ㈿ ㉀ ㉁ ㉂ ㉃ ㉑ ㉒ ㉓ ㉔ ㉕ ㉖ ㉗ ㉘ ㉙ ㉚ ㉛ ㉜ ㉝ ㉞ ㉟ ㉠ ㉡ ㉢ ㉣ ㉤ ㉥ ㉦ ㉧ ㉨ ㉩ ㉪ ㉫ ㉬ ㉭ ㉮ ㉯ ㉰ ㉱ ㉲ ㉳ ㉴ ㉵ ㉶ ㉷ ㉸ ㉹ ㉺ ㉻ ㉿ ㊀ ㊁ ㊂ ㊃ ㊄ ㊅ ㊆ ㊇ ㊈ ㊉ ㊊ ㊋ ㊌ ㊍ ㊎ ㊏ ㊐ ㊑ ㊒ \u0026hellip;\nCJK Compatibility ㌀ ㌁ ㌂ ㌃ ㌄ ㌅ ㌆ ㌇ ㌈ ㌉ ㌊ ㌋ ㌌ ㌍ ㌎ ㌏ ㌐ ㌑ ㌒ ㌓ ㌔ ㌕ ㌖ ㌗ ㌘ ㌙ ㌚ ㌛ ㌜ ㌝ ㌞ ㌟ ㌠ ㌡ ㌢ ㌣ ㌤ ㌥ ㌦ ㌧ ㌨ ㌩ ㌪ ㌫ ㌬ ㌭ ㌮ ㌯ ㌰ ㌱ ㌲ ㌳ ㌴ ㌵ ㌶ ㌷ ㌸ ㌹ ㌺ ㌻ ㌼ ㌽ ㌾ ㌿ ㍀ ㍁ ㍂ ㍃ ㍄ ㍅ ㍆ ㍇ ㍈ ㍉ ㍊ ㍋ ㍌ ㍍ ㍎ ㍏ ㍐ ㍑ ㍒ ㍓ ㍔ ㍕ ㍖ ㍗ ㍘ ㍙ ㍚ ㍛ ㍜ ㍝ ㍞ ㍟ ㍠ ㍡ ㍢ ㍣ ㍤ ㍥ ㍦ ㍧ ㍨ ㍩ ㍪ ㍫ ㍬ ㍭ ㍮ ㍯ ㍰ ㍱ ㍲ ㍳ ㍴ ㍵ ㍶ ㍻ ㍼ ㍽ ㍾ ㍿ ㎀ ㎁ ㎂ ㎃ \u0026hellip;\nCJK Unified Ideographs Extension A 㐀 㐁 㐂 㐃 㐄 㐅 㐆 㐇 㐈 㐉 㐊 㐋 㐌 㐍 㐎 㐏 㐐 㐑 㐒 㐓 㐔 㐕 㐖 㐗 㐘 㐙 㐚 㐛 㐜 㐝 㐞 㐟 㐠 㐡 㐢 㐣 㐤 㐥 㐦 㐧 㐨 㐩 㐪 㐫 㐬 㐭 㐮 㐯 㐰 㐱 㐲 㐳 㐴 㐵 㐶 㐷 㐸 㐹 㐺 㐻 㐼 㐽 㐾 㐿 㑀 㑁 㑂 㑃 㑄 㑅 㑆 㑇 㑈 㑉 㑊 㑋 㑌 㑍 㑎 㑏 㑐 㑑 㑒 㑓 㑔 㑕 㑖 㑗 㑘 㑙 㑚 㑛 㑜 㑝 㑞 㑟 㑠 㑡 㑢 㑣 㑤 㑥 㑦 㑧 㑨 㑩 㑪 㑫 㑬 㑭 㑮 㑯 㑰 㑱 㑲 㑳 㑴 㑵 㑶 㑷 㑸 㑹 㑺 㑻 㑼 㑽 㑾 㑿 \u0026hellip;\nCJK Unified Ideographs 一 丁 丂 七 丄 丅 丆 万 丈 三 上 下 丌 不 与 丏 丐 丑 丒 专 且 丕 世 丗 丘 丙 业 丛 东 丝 丞 丟 丠 両 丢 丣 两 严 並 丧 丨 丩 个 丫 丬 中 丮 丯 丰 丱 串 丳 临 丵 丶 丷 丸 丹 为 主 丼 丽 举 丿 乀 乁 乂 乃 乄 久 乆 乇 么 义 乊 之 乌 乍 乎 乏 乐 乑 乒 乓 乔 乕 乖 乗 乘 乙 乚 乛 乜 九 乞 也 习 乡 乢 乣 乤 乥 书 乧 乨 乩 乪 乫 乬 乭 乮 乯 买 乱 乲 乳 乴 乵 乶 乷 乸 乹 乺 乻 乼 乽 乾 乿 \u0026hellip;\nHangul Syllables 가 각 갂 갃 간 갅 갆 갇 갈 갉 갊 갋 갌 갍 갎 갏 감 갑 값 갓 갔 강 갖 갗 갘 같 갚 갛 개 객 갞 갟 갠 갡 갢 갣 갤 갥 갦 갧 갨 갩 갪 갫 갬 갭 갮 갯 갰 갱 갲 갳 갴 갵 갶 갷 갸 갹 갺 갻 갼 갽 갾 갿 걀 걁 걂 걃 걄 걅 걆 걇 걈 걉 걊 걋 걌 걍 걎 걏 걐 걑 걒 걓 걔 걕 걖 걗 걘 걙 걚 걛 걜 걝 걞 걟 걠 걡 걢 걣 걤 걥 걦 걧 걨 걩 걪 걫 걬 걭 걮 걯 거 걱 걲 걳 건 걵 걶 걷 걸 걹 걺 걻 걼 걽 걾 걿 \u0026hellip;\nCJK Compatibility Ideographs 豈 更 車 賈 滑 串 句 龜 龜 契 金 喇 奈 懶 癩 羅 蘿 螺 裸 邏 樂 洛 烙 珞 落 酪 駱 亂 卵 欄 爛 蘭 鸞 嵐 濫 藍 襤 拉 臘 蠟 廊 朗 浪 狼 郎 來 冷 勞 擄 櫓 爐 盧 老 蘆 虜 路 露 魯 鷺 碌 祿 綠 菉 錄 鹿 論 壟 弄 籠 聾 牢 磊 賂 雷 壘 屢 樓 淚 漏 累 縷 陋 勒 肋 凜 凌 稜 綾 菱 陵 讀 拏 樂 諾 丹 寧 怒 率 異 北 磻 便 復 不 泌 數 索 參 塞 省 葉 說 殺 辰 沈 拾 若 掠 略 亮 兩 凉 梁 糧 良 諒 量 勵 \u0026hellip;\nCJK Compatibility Forms ︰ ︱ ︲ ︳ ︴ ︵ ︶ ︷ ︸ ︹ ︺ ︻ ︼ ︽ ︾ ︿ ﹀ ﹁ ﹂ ﹃ ﹄ ﹅ ﹆ ﹉ ﹊ ﹋ ﹌ ﹍ ﹎ ﹏\n","date":"2018-03-09","permalink":"https://richard-chen93.github.io/post/cjk-unicode-test/","tags":["test"],"title":"CJK Unicode Test"}]